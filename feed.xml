<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:3000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:3000/" rel="alternate" type="text/html" /><updated>2023-03-10T10:21:06-06:00</updated><id>http://localhost:3000/feed.xml</id><entry><title type="html">A page from my research journal</title><link href="http://localhost:3000/2023/02/21/research-page/" rel="alternate" type="text/html" title="A page from my research journal" /><published>2023-02-21T00:00:00-06:00</published><updated>2023-02-21T00:00:00-06:00</updated><id>http://localhost:3000/2023/02/21/research-page</id><content type="html" xml:base="http://localhost:3000/2023/02/21/research-page/">&lt;p&gt;I haven’t seen any blog posts about the process of training
transformers. This is a page from my
research journal: trying to get a from-scratch transformer to do what I
want it to do.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;confusion-1-data-generation&quot;&gt;Confusion 1:
Data generation&lt;/h2&gt;
&lt;p&gt;In order to check that I had set up masking correctly, I decided to
train the transformer to reverse a sequence of digits. For example,
given a sequence &lt;span class=&quot;math inline&quot;&gt;\((1, 2, 3, 4)\)&lt;/span&gt;, I
should get &lt;span class=&quot;math inline&quot;&gt;\((4, 3, 2, 1)\)&lt;/span&gt;. But how
should I set up the training data i.e. what should &lt;span
class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span
class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; be? I flipped through Andrej Karpathy’s
minGPT to find out.&lt;/p&gt;
&lt;p&gt;Turns out, in the example of &lt;span class=&quot;math inline&quot;&gt;\((1, 2, 3,
4)\)&lt;/span&gt;, &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
        x &amp;amp;= (1, 2, 3, 4, 4, 3, 2)\\
        y &amp;amp;= (-1, -1, -1, 4, 3, 2, 1)
    
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is because the transformer is trained to predict the next token,
at &lt;strong&gt;all the positions&lt;/strong&gt; in the input. The &lt;span
class=&quot;math inline&quot;&gt;\(-1\)&lt;/span&gt;’s in the target point indicate the
tokens that should be ignored in the cross-entropy loss. We only care
about reversing the sequence of digits, not about predicting the next
digit in the given sequence.&lt;/p&gt;
&lt;p&gt;The relevant code:&lt;/p&gt;

&lt;img src=&quot;/assets/data/transformer/code_dataloader.png&quot;&gt;

&lt;h2 class=&quot;unnumbered&quot; id=&quot;confusion-2-socrates-data&quot;&gt;Confusion 2:
Socrates data&lt;/h2&gt;
&lt;p&gt;Now it was time to get training on the Socrates data. I had several
concerns at this stage.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;How do I tokenize the Socrates data? I had seen the basics of
tokenizers before, so a quick glance at the OpenAI &lt;a href=&quot;https://github.com/openai/tiktoken&quot;&gt; tiktoken repository&lt;/a&gt;
revealed how to encode and decode tokens. However, I still had to figure
out the vocab size for the tiktoken GPT-2 tokenizer (otherwise,
IndexErrors might crop up during the initial embedding step). After some
snooping around, I found out that the GPT-2 byte-pair encoder (BPE) has a vocab size of
50,927.[1]&lt;/p&gt;&lt;/li&gt;
&lt;div class=&quot;marginnote&quot;&gt;[1] Thanks HuggingFace!&lt;/div&gt; 
&lt;li&gt;&lt;p&gt;What should &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span
class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; be? Unlike the reverse-sequence task,
no tokens have to be ignored for the Socrates task. For example, suppose
the byte-pair encoding for “Socrates: I dare say that you may be
surprised to find” is &lt;span class=&quot;math inline&quot;&gt;\((1, 2, 3, 4, 5, 6, 7,
8, 9, 10, 11)\)&lt;/span&gt;. Then &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
                    x &amp;amp;= (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\\
                    y &amp;amp;= (2, 3, 4, 5, 6, 7, 8, 9, 10, 11).
                
\end{aligned}\]&lt;/span&gt; We want to train the transformer to predict the
next token &lt;strong&gt;at every given position&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How should I write the Dataloader? After figuring out what &lt;span
class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span
class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; should be in the training data, writing
the Dataloader was relatively simple. The relevant code:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&quot;/assets/data/transformer/code_shakespeare_dataloader.png&quot;&gt;

&lt;h2 class=&quot;unnumbered&quot;
id=&quot;confusion-3-coaxing-pseudo-socrates-to-overfit&quot;&gt;Confusion 3: Coaxing
pseudo-Socrates to overfit&lt;/h2&gt;
&lt;p&gt;I first did the stupid thing and pumped the entire Socrates dataset
through the transformer at every epoch, and waited for an extremely long 10 epochs.
However, the loss was barely going down! After each epoch, the average
training loss was going down inconsistently by around &lt;span
class=&quot;math inline&quot;&gt;\(2 \times 10^{-6}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I then did another stupid thing: I increased the learning rate,
pumped the entire Socrates dataset through the transformer, and waited
for another long 10 epochs. Again, the loss barely changed. Believe it
or not, I went back to the learning rate and changed it five more times
- with no improvement in loss degradation, of course. “Maybe adding a
learning rate scheduler or using a different descent algorithm will
help,” I thought. I added a learning rate scheduler and switched the
descent algorithm from stochastic gradient descent to Adam. Did I expect
there to be improvements? Frankly, yes. But wrong I was again - ninety
minutes had gone by at this point.&lt;/p&gt;
&lt;p&gt;I finally landed on the smart thing to do: go trivially small! Don’t
train the transformer on the entire dataset, and instead train
repeatedly on the same datapoint. If the model overfits (i.e. the
validation loss has U-shape while the training loss continues to
decrease), then the model is working correctly. Otherwise, either the
model is hopelessly under-powered, or there is a bug in the code. I
first beefed up the model parameters to make sure that my model was
expressive enough. Then I removed the learning rate scheduler and set
the learning rate to &lt;span class=&quot;math inline&quot;&gt;\(0.1\)&lt;/span&gt;. “There,”
I thought, “it should overfit now” - all I had to do was start training, and watch the
magic.&lt;/p&gt;
&lt;p&gt;No magical sparks flew - the loss barely moved! I suspected that the
loss was not being propogated to the model, so I re-checked the PyTorch
documentation for cross-entropy loss. Lo and behold:&lt;/p&gt;

&lt;img src=&quot;/assets/data/transformer/pytorch.png&quot;&gt;

&lt;p&gt;I removed the last softmax layer, and hit train again. Still, the
loss did not decrease.&lt;/p&gt;
&lt;p&gt;I thought about the model from front-to-back: first, a single data
point gets forward propogated through the model - then the loss is
calculated - then the loss information is propogated backwards, and the
whole cycle starts over again. I had dealt with the latter two steps -
was there something wrong with the first? I checked my DataLoader code,
and found the critical bug: I was shuffling the DataLoader at every
step, so I was feeding in different points to the model! I quickly
cooked up a debug flag that prevented the DataLoader from shuffling at
every call.&lt;/p&gt;
&lt;div class=&quot;marginnote&quot;&gt;&lt;img src=&quot;/assets/data/transformer/success_baby.gif&quot; style=&quot;width: 40%; &quot;&gt;&lt;/div&gt; 
&lt;p&gt;Hands clasped in prayer, I hit train again, and got the following curves:&lt;/p&gt;

&lt;img src=&quot;/assets/data/transformer/train_loss.png&quot;&gt;
&lt;img src=&quot;/assets/data/transformer/val_loss.png&quot;&gt;

&lt;p&gt;Thus we conclude this confusion stage with three important
lessons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Read the documentation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When debugging, reduce - reduce - reduce to the trivial
case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When debugging machine learning models, remove as much randomness
as possible! This is akin to statistical exercises - it’s imperative to
keep track of which variables are stochastic and which are
deterministic.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class=&quot;unnumbered&quot;
id=&quot;confusion-4-training-pseudo-socrates&quot;&gt;Confusion 4: Training
pseudo-Socrates&lt;/h2&gt;
&lt;p&gt;With no outright bugs present in the code, I turned to actually
training the transformer. I realized that I had zero clue of how machine
learning optimization algorithms actually work; having taken a machine
learning class, I knew about stochastic gradient descent and momentum, but the details behind fancy (?) methods like Adam were
unbeknownst to me. Nor did I know in-depth about regularization methods.&lt;/p&gt;
&lt;p&gt;This journey resulted in my &lt;a href=&quot;https://alikiki.github.io/2023/02/20/deep-learning/&quot;&gt;deep learning optimization glossary&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With the newfound knowledge, I played
with the learning rate and the batch size, and tried out different
regularization methods like early stopping and dropout.&lt;/p&gt;
&lt;p&gt;For a concrete example, check out the experiment that I tried to see
the effects of dropout rate. Intuitively, the effective complexity of
the model should increase as the dropout rate decreases. If the dropoput
rate is 0.0, then all the units are always included. On the other hand,
if the dropout rate is 1.0, then all the units are always zeroed out.
Because I added dropout layers after having finished the base
transformer code, I decided to run some trivial overfitting checks: if
the model with maximum dropout rate learns better than the one with the
minimum dropout rate, then something must be wrong. Indeed, the
empirical results match with the intuition:&lt;/p&gt;

&lt;img src=&quot;/assets/data/transformer/dropout_train.png&quot;&gt;

&lt;h2 class=&quot;unnumbered&quot; id=&quot;confused-but-less-so&quot;&gt;Confused, but now less
so&lt;/h2&gt;
&lt;p&gt;My final model has ~17.7 million parameters, run with the Adam optimizer with initial learning rate 1e-3. Trained just under three hours on my MacBook CPU, it achieves a loss of ~0.37. Here are some generations, prompted with Socrates text:&lt;/p&gt;


&lt;img src=&quot;/assets/data/transformer/result1.png&quot;&gt;
&lt;img src=&quot;/assets/data/transformer/result2.png&quot;&gt;

Not good, but also not bad.

&lt;h2 class=&quot;unnumbered&quot; id=&quot;appendix&quot;&gt;Appendix: Transformer Architecture&lt;/h2&gt;
&lt;p&gt;Decoder-only transformers have a pretty simple architecture. I’ve put
a brief outline below.&lt;/p&gt;
&lt;p&gt;Single-head transformers take an input and trace the steps written
below. Note that all the vectors here are row vectors, because for some
reason the machine learning community enjoys row vectors more than
column vectors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take an input vector &lt;span class=&quot;math inline&quot;&gt;\(x \in
\mathbb{R}^n\)&lt;/span&gt;. Let &lt;span class=&quot;math inline&quot;&gt;\(e&amp;#39;(x) \in
\mathbb{R}^{n \times d}\)&lt;/span&gt; be the embedding of &lt;span
class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;, and &lt;span
class=&quot;math inline&quot;&gt;\(p&amp;#39;(x) \in \mathbb{R}^{n \times d}\)&lt;/span&gt; be
the positional encoding of &lt;span
class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use &lt;span class=&quot;math inline&quot;&gt;\(e_x = e&amp;#39;(x) +
p&amp;#39;(x)\)&lt;/span&gt; as the final embedding of &lt;span
class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define &lt;span class=&quot;math inline&quot;&gt;\(q(x), k(x), v(x) \in
\mathbb{R}^v\)&lt;/span&gt; as the query, key, and value vectors of &lt;span
class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;, respectively. More specifically, we
can define &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
                    q(x) = e_x W_Q, k(x) = e_x W_K, v(x) = e_x W_V,
\text{ where } W_Q, W_K, W_V \in \mathbb{R}^{d \times v}.
                
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define the attention function &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
                    A_{W_Q, W_K, W_V}(x) =
\text{softmax}\left(\frac{q(x) k(x)^T}{\sqrt{v}}\right) v(x).
                
\end{aligned}\]&lt;/span&gt; Notice that the attention function is
parametrized by the query, key, and value matrices.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Project &lt;span class=&quot;math inline&quot;&gt;\(A_{W_Q, W_K, W_V}(x)\)&lt;/span&gt;
back into &lt;span class=&quot;math inline&quot;&gt;\(d\)&lt;/span&gt;-dimensional space with
the matrix &lt;span class=&quot;math inline&quot;&gt;\(W_O \in \mathbb{R}^{v \times
d}\)&lt;/span&gt; i.e. take &lt;span class=&quot;math inline&quot;&gt;\(A_{W_Q, W_K, W_V}(x)
\cdot W_O\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define &lt;span class=&quot;math inline&quot;&gt;\(l_{11}(x) =
\text{LayerNorm}(e_x + A_{W_Q, W_K, W_V}(x) \cdot
W_O)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Push &lt;span class=&quot;math inline&quot;&gt;\(l_{11}(x)\)&lt;/span&gt; through a
feedforward neural network with ReLU activation to get &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            l_{12}(x) = \max(0, l_{11}(x) W_1 + b) W_2 + b_2,
        
\end{aligned}\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_1 \in
\mathbb{R}^{d \times h}, W_2 \in \mathbb{R}^{h \times d}\)&lt;/span&gt; and
&lt;span class=&quot;math inline&quot;&gt;\(h\)&lt;/span&gt; is the hidden layer
dimension.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define &lt;span class=&quot;math inline&quot;&gt;\(l_{1}(x) =
\text{LayerNorm}(l_{11}(x) + l_{12}(x))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat steps 3 through 7 for &lt;span
class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; layers. Denote the final output as
&lt;span class=&quot;math inline&quot;&gt;\(l_k(x)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Project &lt;span class=&quot;math inline&quot;&gt;\(l_k(x)\)&lt;/span&gt; to the vocab
size dimension i.e. take &lt;span class=&quot;math inline&quot;&gt;\(l_k(x)
W_{\text{vocab}} + b_{\text{vocab}}\)&lt;/span&gt;, where &lt;span
class=&quot;math inline&quot;&gt;\(W_{\text{vocab}} \in \mathbb{R}^{d \times
\text{vocab size}}, b \in \mathbb{R}^{\text{vocab
size}}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take the softmax to get &lt;span class=&quot;math inline&quot;&gt;\(y =
\text{softmax}(l_k(x) W_{\text{vocab}} +
b_{\text{vocab}})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Multi-head attention isn’t much different - we just need to numerate
our query, key, and value matrices. Assuming &lt;span
class=&quot;math inline&quot;&gt;\(h\)&lt;/span&gt; heads, we define &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
    q_i(x) = e_x W_Q^i, \text{ }k_i(x) = e_x W_K^i, \text{ }v_i(x) = e_x
W_V^i.
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then our attention function becomes &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
    A_{W_Q^i, W_K^i, W_V^i}(x) = \text{softmax}\left(\frac{q_i(x)
k_i(x)^T}{\sqrt{v}}\right) v_i(x).
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To do step 5, we take &lt;span class=&quot;math inline&quot;&gt;\(W_O \in
\mathbb{R}^{(h \cdot v) \times d}\)&lt;/span&gt;, and apply it to the
concatenated matrix to get &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
    \begin{bmatrix}
        A_{W_Q^1, W_K^1, W_V^1}(x) \mid &amp;amp; \cdots &amp;amp; \mid
A_{W_Q^h, W_K^h, W_V^h}(x)
    \end{bmatrix} W_O.
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">I haven’t seen any blog posts about the process of training transformers. This is a page from my research journal: trying to get a from-scratch transformer to do what I want it to do. Confusion 1: Data generation In order to check that I had set up masking correctly, I decided to train the transformer to reverse a sequence of digits. For example, given a sequence \((1, 2, 3, 4)\), I should get \((4, 3, 2, 1)\). But how should I set up the training data i.e. what should \(x\) and \(y\) be? I flipped through Andrej Karpathy’s minGPT to find out. Turns out, in the example of \((1, 2, 3, 4)\), \[\begin{aligned} x &amp;amp;= (1, 2, 3, 4, 4, 3, 2)\\ y &amp;amp;= (-1, -1, -1, 4, 3, 2, 1) \end{aligned}\] This is because the transformer is trained to predict the next token, at all the positions in the input. The \(-1\)’s in the target point indicate the tokens that should be ignored in the cross-entropy loss. We only care about reversing the sequence of digits, not about predicting the next digit in the given sequence. The relevant code:</summary></entry><entry><title type="html">Deep Learning Optimization Glossary</title><link href="http://localhost:3000/2023/02/20/deep-learning/" rel="alternate" type="text/html" title="Deep Learning Optimization Glossary" /><published>2023-02-20T00:00:00-06:00</published><updated>2023-02-20T00:00:00-06:00</updated><id>http://localhost:3000/2023/02/20/deep-learning</id><content type="html" xml:base="http://localhost:3000/2023/02/20/deep-learning/">&lt;a href=&quot;#adagrad&quot;&gt;AdaGrad&lt;/a&gt; | 
&lt;a href=&quot;#adam&quot;&gt;Adam&lt;/a&gt; | 
&lt;a href=&quot;#batch-optimization&quot;&gt;Batch Optimization&lt;/a&gt; | 
&lt;a href=&quot;#bias&quot;&gt;Bias&lt;/a&gt; | 
&lt;a href=&quot;#bias-variance-decomposition&quot;&gt;Bias-variance decomposition&lt;/a&gt; | 
&lt;a href=&quot;#bootstrap-aggregating&quot;&gt;Bootstrap aggregating&lt;/a&gt; | 
&lt;a href=&quot;#dropout&quot;&gt;Dropout&lt;/a&gt; | 
&lt;a href=&quot;#early-stopping&quot;&gt;Early stopping&lt;/a&gt; | 
&lt;a href=&quot;#empirical-risk-minimization&quot;&gt;Empirical risk minimization&lt;/a&gt; | 
&lt;a href=&quot;#gradient&quot;&gt;Gradient&lt;/a&gt; | 
&lt;a href=&quot;#gradient-descent&quot;&gt;Gradient descent&lt;/a&gt; | 
&lt;a href=&quot;#hessian&quot;&gt;Hessian&lt;/a&gt; | 
&lt;a href=&quot;#l1-regularization&quot;&gt;L1 Regularization&lt;/a&gt; | 
&lt;a href=&quot;#l2-regularization&quot;&gt;L2 Regularization&lt;/a&gt; | 
&lt;a href=&quot;#label-smoothing&quot;&gt;Label smoothing&lt;/a&gt; | 
&lt;a href=&quot;#learning-problem&quot;&gt;Learning problem&lt;/a&gt; | 
&lt;a href=&quot;#mini-batch-optimization&quot;&gt;Mini-batch optimization&lt;/a&gt; | 
&lt;a href=&quot;#momentum&quot;&gt;Momentum&lt;/a&gt; | 
&lt;a href=&quot;#nesterov-momentum&quot;&gt;Nesterov momentum&lt;/a&gt; | 
&lt;a href=&quot;#noise-robustness&quot;&gt;Noise Robustness&lt;/a&gt; | 
&lt;a href=&quot;#rmsprop&quot;&gt;RMSProp&lt;/a&gt; | 
&lt;a href=&quot;#stochastic-gradient-descent-sgd&quot;&gt;Stochastic gradient descent (SGD)&lt;/a&gt; | 
&lt;a href=&quot;#validation-set&quot;&gt;Validation set&lt;/a&gt; | 

&lt;h2 class=&quot;unnumbered&quot; id=&quot;adagrad&quot;&gt;AdaGrad&lt;/h2&gt;
&lt;p&gt;AdaGrad belongs to a class of gradient descent algorithms with
adaptive learning rates. The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose a learning rate &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt; and initial parameters &lt;span
class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;. Do the following until a
stopping criterion is met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take a mini-batch &lt;span class=&quot;math inline&quot;&gt;\(\{(x^{(i)},
y^{(i)})\}_{i=1,\cdots, m}\)&lt;/span&gt; from the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient estimate &lt;span class=&quot;math display&quot;&gt;\[g_t =
\frac{1}{m} \sum_{i=1}^m \nabla_{\theta} L(f(x^{(i)}; \theta_t),
y^{(i)})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aggregate the outer product of all the previous gradient
estimates &lt;span class=&quot;math inline&quot;&gt;\(G_t = \sum_{i=0}^{t} g_i
g_i^T\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the parameter estimate &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_t - \epsilon
G_t^{-1/2} g_t.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Instead of using &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; to update
the parameters, it is common to use &lt;span
class=&quot;math inline&quot;&gt;\(\text{diag}(G_t + \delta I)\)&lt;/span&gt; with small
&lt;span class=&quot;math inline&quot;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt; to avoid singularity
and simplify computation; hence the update rule becomes &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_t - \epsilon \cdot
\text{diag}(G_t + \delta I)^{-1/2} g_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To see why this is an adaptive learning algorithm, observe &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon \cdot \text{diag}(G_t + \delta
I)^{-1/2}\)&lt;/span&gt;. The diagonal of &lt;span class=&quot;math inline&quot;&gt;\(G_t +
\delta I\)&lt;/span&gt; consists only of positive elements, and each element
increases with each iteration. This means that the diagonal elements of
&lt;span class=&quot;math inline&quot;&gt;\(\text{diag}(G_t + \delta I)^{-1/2}\)&lt;/span&gt;
will shrink closer and closer to zero with each iteration - adapting the
learning rate.&lt;/p&gt;
&lt;p&gt;The primary disadvantage to AdaGrad is that the rate of learning rate
decay depends on the path of the gradient steps. The RMSProp algorithm
aims to quell this weakness by exponentially weighting the past gradient
aggregations.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;adam&quot;&gt;Adam&lt;/h2&gt;
&lt;p&gt;Adam belongs to a class of gradient descent algorithms with adaptive
learning rates. The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose a learning rate &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;, two decay rates &lt;span
class=&quot;math inline&quot;&gt;\(\beta_0, \beta_1 \in [0,1)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose initial first and second moment estimates of the gradient
&lt;span class=&quot;math inline&quot;&gt;\(m_0 = v_0 = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose initial parameters &lt;span
class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do the following until a stopping criterion is met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take a mini-batch &lt;span class=&quot;math inline&quot;&gt;\(\{(x^{(i)},
y^{(i)})\}_{i=1,\cdots, m}\)&lt;/span&gt; from the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient estimate &lt;span class=&quot;math display&quot;&gt;\[g_t =
\frac{1}{m} \sum_{i=1}^m \nabla_{\theta} L(f(x^{(i)}; \theta_t),
y^{(i)})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the first moment estimate &lt;span class=&quot;math inline&quot;&gt;\(m_t
\leftarrow \beta_0 \cdot m_{t-1} + (1 - \beta_0) \cdot
g_t\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the second raw moment estimate &lt;span
class=&quot;math inline&quot;&gt;\(v_t \leftarrow \beta_1 \cdot v_{t-1} + (1 -
\beta_1) \cdot g_t^2\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Correct the first moment estimate &lt;span
class=&quot;math inline&quot;&gt;\(m_t&amp;#39; \leftarrow
\frac{m_t}{1-\beta_0^t}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Correct the second raw moment estimate &lt;span
class=&quot;math inline&quot;&gt;\(v_t&amp;#39; \leftarrow \frac{v_t}{1 -
\beta_1^t}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the parameter estimate &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_{t} -
\frac{\epsilon}{\delta + \sqrt{v_t&amp;#39;}} \cdot m_t&amp;#39;\)&lt;/span&gt; for
small &lt;span class=&quot;math inline&quot;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is easy to see that Adam is an adaptive learning algorithm - one
need only look at the update rule &lt;span class=&quot;math inline&quot;&gt;\(v_t
\leftarrow \beta_1 \cdot v_{t-1} + (1 - \beta_1) \cdot g_t^2\)&lt;/span&gt;
and notice that this sum can only increase with each iteration.&lt;/p&gt;
&lt;p&gt;The exponential weighted sum of the gradient provides a biased
estimate of the first moment of the stochastic gradient, taken from some
true gradient distribution. Likewise, the exponential weighted sum of
the squared gradients gives a biased esimate of second raw moment of the
stochastic gradient. The biased estimates are then corrected in order to
update the parameters. To see how the bias is corrected by a simple
multiplication, we have &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            \mathbb{E}(m_t) &amp;amp;= \mathbb{E}\left[(1 - \beta_0)
\sum_{i=1}^{t} \beta_0^i \cdot g_{t-i}\right]\\
            &amp;amp;= \mathbb{E}(g_t) \cdot (1 - \beta_0^t) + C.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&quot;math inline&quot;&gt;\(\{g_t\}\)&lt;/span&gt; is stationary, the
&lt;span class=&quot;math inline&quot;&gt;\(C = 0\)&lt;/span&gt;. Otherwise, &lt;span
class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt; can be kept small because the decay
rate &lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt; can be chosen to
heavily penalize gradients seen in the earlier steps. Therefore, the
first moment &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{E}(g_t)\)&lt;/span&gt; can be
obtained by a corrected &lt;span class=&quot;math inline&quot;&gt;\((1 - \beta_0^t)^{-1}
\cdot \mathbb{E}(m_t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;batch-optimization&quot;&gt;Batch optimization&lt;/h2&gt;
&lt;p&gt;Batch optimization uses the entire training set for each gradient
step. It is very computationally expensive for large datasets since the
model must evaluate every single data point for every step. However, it
gives the “true” empirical gradient.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;bias&quot;&gt;Bias&lt;/h2&gt;
&lt;p&gt;Suppose we have an estimate &lt;span
class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; of a true parameter &lt;span
class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;. Note that the estimator &lt;span
class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; is stochastic. The bias of
&lt;span class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; is defined as &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            \text{Bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) -
\theta.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&quot;math inline&quot;&gt;\(\text{Bias}(\hat{\theta}) =
0\)&lt;/span&gt;, then &lt;span class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; is
called an unbiased estimator.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;bias-variance-decomposition&quot;&gt;Bias-variance
decomposition&lt;/h2&gt;
&lt;p&gt;Suppose we have an estimate &lt;span
class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; of a true parameer &lt;span
class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;. Not e that the estimator &lt;span
class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; is stochastic. Then the mean
squared-error of &lt;span class=&quot;math inline&quot;&gt;\(\hat{\theta}\)&lt;/span&gt; can
be decomposed as follows: &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            \text{MSE}(\hat{\theta}) &amp;amp;= \mathbb{E}[(\hat{\theta} -
\theta)^2]\\
            &amp;amp;= \text{Bias}(\hat{\theta})^2 +
\text{Var}(\hat{\theta}).
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;bootstrap-aggregating&quot;&gt;Bootstrap
aggregating&lt;/h2&gt;
&lt;p&gt;Bootstrap aggregating (or “bagging”) is a model averaging method. The
bagging algorithm is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Suppose the training dataset has &lt;span
class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; data points. For &lt;span
class=&quot;math inline&quot;&gt;\(i \in \{1, 2, \cdots, k\}\)&lt;/span&gt;, create
training set &lt;span class=&quot;math inline&quot;&gt;\(T_i\)&lt;/span&gt; by sampling with
replacement from the training set &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;
times.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Train model &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; with training
set &lt;span class=&quot;math inline&quot;&gt;\(T_i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Average the outputs of the results from each model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A quick argument for why bagging works: take &lt;span
class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; models, and suppose that model &lt;span
class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; makes an error &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon_i\)&lt;/span&gt;, where &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon \sim \mathcal{N}(0, vI)\)&lt;/span&gt;. Then
the expected squared error from bagging is &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            \mathbb{E}\left(k^{-1} \sum_{i} \epsilon_i\right)^2 = k^{-2}
\cdot kv = \frac{v}{k}.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence if the errors are uncorrelated, then we have cut down on the
expected squared error just by averaging.&lt;/p&gt;
&lt;p&gt;Ensemble models are a good example of trading off performance for
compute + memory demands.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;dropout&quot;&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a regularization method that cheaply approximates bagging.
It is also a method of applying noise to input and hidden units. The
dropout algorithm works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;With each mini-batch loading, randomly select a binary vector
&lt;span class=&quot;math inline&quot;&gt;\(\mu \in \{0,1\}^d\)&lt;/span&gt;, where &lt;span
class=&quot;math inline&quot;&gt;\(d\)&lt;/span&gt; is the number of input and hidden units
in the neural network. This vector represents which units to
include/exclude in the model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run forward and backward propogation with &lt;span
class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; applied element-wise to the
corresponding units, and report the cost &lt;span
class=&quot;math inline&quot;&gt;\(J(\theta, \mu)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal of dropout is to get an unbiased estimate of &lt;span
class=&quot;math inline&quot;&gt;\(\mathbb{E}_{\mu} J(\theta, \mu)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dropout really is similar to bagging - a “new” model is created every
time we apply a new binary vector &lt;span
class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;, and each model experiences a
separate “training set” (which is represented by the mini-batch). On the
other hand, the key difference between bagging models and dropout models
is that dropout models share model parameters with the “parent” model
i.e. the neural network with &lt;span class=&quot;math inline&quot;&gt;\(\mu = [1, 1,
\cdots, 1]\)&lt;/span&gt;. Furthermore, aggregating the results of each
dropout model is not as clear as averaging out the models results.&lt;/p&gt;
&lt;p&gt;Because of the included stochasticity, the output of the model at
test time should intuitively be the expected output during training
time. Suppose a unit is included in a dropout model with probability
&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;, and the weights branching from
this unit is &lt;span class=&quot;math inline&quot;&gt;\(\textbf{w}\)&lt;/span&gt;. Averaged
over all dropout combinations, the expected weights branching from unit
is &lt;span class=&quot;math inline&quot;&gt;\(p \textbf{w}\)&lt;/span&gt; because the weights
are zero if the unit is excluded. Hence, we can combine the results of
the different dropout models by multiplying each of the weights by the
probability of inclusion. Alternatively, we can multiply the weights by
&lt;span class=&quot;math inline&quot;&gt;\(1/p\)&lt;/span&gt; during training, and use the
vanilla weights during testing.&lt;/p&gt;
&lt;p&gt;Although dropout applies a random binary vector, using a random
vector of real values e.g. &lt;span class=&quot;math inline&quot;&gt;\(\mu \sim N(1,
I)\)&lt;/span&gt; can yield similar results. This supports the idea that
dropout is like adding noise to the neural network units.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;early-stopping&quot;&gt;Early stopping&lt;/h2&gt;
&lt;p&gt;Early stopping is an implicit regularization method where you stop
the training process just before the model starts to overfit. The early
stopping meta-algorithm works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For an epoch &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;, train the
model and retrieve an evaluation score by testing on a validation
set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the evaluation score has improved from the last epoch, save
the model parameters and the epoch number. Continue to the next
epoch.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the evaluation score has not improved for more than &lt;span
class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; epochs, then exit. (&lt;span
class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; is called the “patience”)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the evaluation score has not improved, but it has not been
more than &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; epochs since it last
improved, continue to the next epoch.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The idea behind early stopping as an implicit regularizer is that the
number of training steps is a hyperparameter - this means that stopping
training after a variable number of steps produces a variety of models.
Given that the model is complex enough to sufficiently learn the task,
the validation curve should be U-shaped as a function of the number of
training steps. By early stopping, we are constraining the effective
capacity of the model.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;empirical-risk-minimization&quot;&gt;Empirical risk
minimization&lt;/h2&gt;
&lt;p&gt;Empirical risk minimization is the framework that sets up the details
of the learning problem. Our ultimate goal is to minimize the expected
generalization error (otherwise known as the risk): with a data
generating distribution &lt;span
class=&quot;math inline&quot;&gt;\(p_{\text{data}}\)&lt;/span&gt;, the expected
generalization error is &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            J^*(\theta) = \mathbb{E}_{(x,y) \sim p_{\text{data}}} L(f(x;
\theta), y).
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because we do not have access to &lt;span
class=&quot;math inline&quot;&gt;\(p_{\text{data}}\)&lt;/span&gt;, the risk is intractable.
Instead, we choose to optimize the empirical risk, which is the expected
loss on the training set. Since the training set can be seen as being
drawn from the empirical distribution &lt;span
class=&quot;math inline&quot;&gt;\(\hat{p}_{\text{data}}\)&lt;/span&gt;, the expected loss
on the training set is defined as &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            J(\theta) &amp;amp;= \mathbb{E}_{(x,y) \sim
\hat{p}_{\text{data}}} L(f(x; \theta), y) \\
            &amp;amp;= \frac{1}{n} \sum_{i=1}^n L(f(x_i; \theta), y_i).
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, if the original loss function &lt;span
class=&quot;math inline&quot;&gt;\(L\)&lt;/span&gt; does not have nice properties (not
continuous e.g. 0-1 loss), a surrogate loss function is used as a proxy
during training. For example, negative log likelihood loss is used as a
surrogate for 0-1 loss. Regardless, the framework still holds - the goal
is to find the parameters that minimize the empirical risk with the hope
that true risk will also be minimized.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;gradient&quot;&gt;Gradient&lt;/h2&gt;
&lt;p&gt;Given a function &lt;span class=&quot;math inline&quot;&gt;\(f: \mathbb{R}^n \to
\mathbb{R}\)&lt;/span&gt;, the gradient of &lt;span
class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt; at point &lt;span
class=&quot;math inline&quot;&gt;\(x_0\)&lt;/span&gt; is denoted as &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            \nabla f(x_0) = \begin{bmatrix}
                \frac{\partial}{\partial x_1} f(x_0) &amp;amp; \cdots &amp;amp;
\frac{\partial}{\partial x_n} f(x_0)
            \end{bmatrix}^T.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At a point &lt;span class=&quot;math inline&quot;&gt;\(x_0\)&lt;/span&gt;, the gradient
points in the direction of steepest ascent. This can be shown through
the directional directive. We want to find a unit vector &lt;span
class=&quot;math inline&quot;&gt;\(u^*\)&lt;/span&gt; such that &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            u^* &amp;amp;= \arg \max_{||u||_2 = 1} \lim_{h \to 0}
\frac{f(x_0  + h u) - f(x_0)}{h} \\
            &amp;amp;= \arg \max_{||u||_2 = 1} (u^T) (\nabla f(x_0))\\
            &amp;amp;= \arg \max_{||u||_2 = 1} ||u||_2 ||\nabla f(x_0)||_2
\cos \theta.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&quot;math inline&quot;&gt;\(\nabla f(x_0)\)&lt;/span&gt; is
unaffected by &lt;span class=&quot;math inline&quot;&gt;\(u\)&lt;/span&gt; and &lt;span
class=&quot;math inline&quot;&gt;\(||u||_2 = 1\)&lt;/span&gt;, the above optimization is
equivalent to &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            u^* = \arg \max_{||u||_2 = 1} \cos \theta.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is maximized when &lt;span class=&quot;math inline&quot;&gt;\(\theta =
0\)&lt;/span&gt;, so &lt;span class=&quot;math inline&quot;&gt;\(u^*\)&lt;/span&gt; is the
normalized vector of &lt;span class=&quot;math inline&quot;&gt;\(\nabla
f(x_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In contrast, &lt;span class=&quot;math inline&quot;&gt;\(u_* = \arg \min_{||u||_2 =
1} ||u||_2 ||\nabla f(x_0)||_2 \cos \theta = - u^*.\)&lt;/span&gt; Hence the
gradient points in the direction of steepest ascent, and the opposite
direction points in the direction of steepest descent.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h2&gt;
&lt;p&gt;Gradient descent is an iterative algorithm for solving a minimization
problem. Typically, we are trying to find a point &lt;span
class=&quot;math inline&quot;&gt;\(x_*\)&lt;/span&gt; such that &lt;span
class=&quot;math inline&quot;&gt;\(x_* \arg \min_{x} f(x)\)&lt;/span&gt; for some objective
function &lt;span class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt;. The core idea of
gradient descent depends on the fact that the negative of the gradient
points in the direction of steepest descent. Hence the algorithm works
by iteratively stepping in the opposite direction to the gradient, using
a step size &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;: &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            x^{(i+1)} \leftarrow x^{(i)} - \epsilon \cdot \nabla
f(x^{(i)}).
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The only parameter that we can choose is the step size &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;. To derive further insights
about the step size, we can use the second order Taylor approximation.
For notational convenience, we denote &lt;span class=&quot;math inline&quot;&gt;\(\nabla
f(x^{(i)})\)&lt;/span&gt; as &lt;span class=&quot;math inline&quot;&gt;\(g\)&lt;/span&gt;: &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            f(x^{(i+1)}) &amp;amp;= f(x^{(i)} - \epsilon \cdot g)\\
            &amp;amp;\approx f(x^{(i)}) - \epsilon \cdot g^T g +
\frac{\epsilon^2}{2} \cdot g^T H g.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Keep in mind that the goal is to iteratively find the
&lt;strong&gt;minimum&lt;/strong&gt; of &lt;span class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt;.
Depending on the value of &lt;span class=&quot;math inline&quot;&gt;\(g^T H g\)&lt;/span&gt;,
there are a few possible behaviors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&quot;math inline&quot;&gt;\(g^T H g \leq 0\)&lt;/span&gt;, then
taking &lt;span class=&quot;math inline&quot;&gt;\(\epsilon \to \infty\)&lt;/span&gt; means
that the cost &lt;span class=&quot;math inline&quot;&gt;\(f({x^{(i+1)}})\)&lt;/span&gt; will
be infinitely smaller than &lt;span
class=&quot;math inline&quot;&gt;\(f(x^{(i)})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&quot;math inline&quot;&gt;\(g^T H g &amp;gt; 0\)&lt;/span&gt;, then
taking the partial derivative of the above Taylor approximation with
respect to &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;, we get the
optimal step size (the step size that gives the largest decrease going
from &lt;span class=&quot;math inline&quot;&gt;\(x^{(i)} \to x^{(i+1)}\)&lt;/span&gt;) &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
                    \epsilon^* = \frac{g^T g}{g^T H g}.
                
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;hessian&quot;&gt;Hessian&lt;/h2&gt;
&lt;p&gt;Given a function &lt;span class=&quot;math inline&quot;&gt;\(f: \mathbb{R}^n \to
\mathbb{R}\)&lt;/span&gt; with second-order partial derivatives, the Hessian
at point &lt;span class=&quot;math inline&quot;&gt;\(x_0\)&lt;/span&gt; is a matrix &lt;span
class=&quot;math inline&quot;&gt;\(H(x_0) \in \mathbb{R}^{n \times n}\)&lt;/span&gt; such
that &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            H(x_0) = \begin{bmatrix}
                \frac{\partial^2}{\partial x_1^2} f(x_0) &amp;amp; \cdots
&amp;amp; \frac{\partial^2}{\partial x_1 x_n} f(x_0)\\
                \vdots &amp;amp; \ddots &amp;amp; \vdots\\
                \frac{\partial^2}{\partial x_n x_1} f(x_0)&amp;amp; \cdots
&amp;amp; \frac{\partial^2}{\partial x_n^2} f(x_0)
            \end{bmatrix}.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;span class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt; usually has
continuous second-order partial derivatives, which means that the
Hessian is a real symmetric matrix. Hence the Hessian is unitarily
diagonalizable (eigenbasis consists of orthonormal vectors) by the
spectral theorem.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;l1-regularization&quot;&gt;L1 Regularization&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(L^1\)&lt;/span&gt; regularization is when you
add an &lt;span class=&quot;math inline&quot;&gt;\(L^1\)&lt;/span&gt; penalty term to the cost
function. In the case of simple linear regression, where &lt;span
class=&quot;math inline&quot;&gt;\(\hat{y} = X\hat{\beta}\)&lt;/span&gt;, &lt;span
class=&quot;math inline&quot;&gt;\(L^1\)&lt;/span&gt; regularization yields the cost
function &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            J(\hat{\beta}) = ||y - X \hat{\beta}||_2^2 + \lambda
||\hat{\beta}||_1.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the case of a fully connected neural network with &lt;span
class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; hidden layers, we have &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            J(\theta) &amp;amp;= J(W^{(1)}, b^{(1)}, \cdots, W^{(l)},
b^{(l)})\\
            &amp;amp;= ||y - f(X)||_2^2 + \lambda \sum_{i=1}^{l}
||W^{(i)}||_1.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall that the matrix &lt;span class=&quot;math inline&quot;&gt;\(1\)&lt;/span&gt;-norm
goes by the nickname “max column sum”.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;l2-regularization&quot;&gt;L2 Regularization&lt;/h2&gt;
&lt;span class=&quot;math inline&quot;&gt;\(L^2\)&lt;/span&gt; regularization is when you add
an &lt;span class=&quot;math inline&quot;&gt;\(L^2\)&lt;/span&gt; penalty term to the cost
function. In the case of simple linear regression, where &lt;span
class=&quot;math inline&quot;&gt;\(\hat{y} = X\hat{\beta}\)&lt;/span&gt;, &lt;span
class=&quot;math inline&quot;&gt;\(L^2\)&lt;/span&gt; regularization yields the cost
function &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            J(\hat{\beta}) = ||y - X \hat{\beta}||_2^2 + \lambda
||\hat{\beta}||_2^2.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the case of a fully connected neural network with &lt;span
class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; hidden layers, we have &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            J(\theta) &amp;amp;= J(W^{(1)}, b^{(1)}, \cdots, W^{(l)},
b^{(l)})\\
            &amp;amp;= ||y - f(X)||_2^2 + \lambda \sum_{i=1}^{l}
||W^{(i)}||_F^2.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall that &lt;span class=&quot;math inline&quot;&gt;\(|| \cdot ||_F\)&lt;/span&gt; is the
Frobenius norm: &lt;span class=&quot;math inline&quot;&gt;\(||X||_F^2 =
||\text{Vec}(X)||_2^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is worth noting that adding small-variance noise to the input
vectors has an &lt;span class=&quot;math inline&quot;&gt;\(L^2\)&lt;/span&gt; regularization
effect. Given points sampled i.i.d. from a distribution &lt;span
class=&quot;math inline&quot;&gt;\(p(x,y)\)&lt;/span&gt;, where &lt;span
class=&quot;math inline&quot;&gt;\(x \in \mathbb{R}^d, y \in \mathbb{R}^c\)&lt;/span&gt;,
we have the cost function &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            E(\hat{f}) &amp;amp;= \frac{1}{2} \iint ||y - \hat{f}(x)||_2^2
\cdot p(x,y) \text{ }dx \text{ }dy\\
            &amp;amp;= \frac{1}{2} \sum_{k=1}^c \iint (y_k - \hat{f}_k(x))^2
\cdot p(y_k | x) \cdot p(x) \text{ }dx \text{ }dy_k.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&quot;math inline&quot;&gt;\(\zeta \sim \tilde{p}\)&lt;/span&gt; is
a &lt;span class=&quot;math inline&quot;&gt;\(d\)&lt;/span&gt;-dimensional random vector,
where &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            \mathbb{E}(\zeta) = \vec{0} \text{ and } \int \zeta_i
\zeta_j \tilde{p}(\zeta) \text{ }d \zeta = \eta^2 \delta_{ij}.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&quot;math inline&quot;&gt;\(\delta_{ij} = 1\)&lt;/span&gt; if
&lt;span class=&quot;math inline&quot;&gt;\(i=j\)&lt;/span&gt;, and &lt;span
class=&quot;math inline&quot;&gt;\(0\)&lt;/span&gt; otherwise. We perturb the inputs, and
evaluate to get a cost function &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            \tilde{E}(\hat{f}) = \frac{1}{2} \iint \sum_{k=1}^c \int
(y_k - \hat{f}_k(x + \zeta))^2 \cdot p(y_k | x) \cdot p(x) \cdot
\tilde{p}(\zeta) \text{ }dx \text{ }dy_k \text{ }d\zeta.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given sufficiently small &lt;span class=&quot;math inline&quot;&gt;\(\eta\)&lt;/span&gt;,
we can approximate &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}_k(x +
\zeta)\)&lt;/span&gt; with a second order Taylor approximation. Denoting the
Hessian of &lt;span class=&quot;math inline&quot;&gt;\(\hat{f}_k\)&lt;/span&gt; evaluated at
&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; as &lt;span
class=&quot;math inline&quot;&gt;\(H_k\)&lt;/span&gt;, we have: &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            \hat{f}_k(x + \zeta) \approx \hat{f}_k(x) + \zeta^T \nabla
\hat{f}_k(x) + \frac{1}{2}\zeta^T H_k \zeta.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting the approximation into &lt;span
class=&quot;math inline&quot;&gt;\(\tilde{E}(\hat{f})\)&lt;/span&gt;, we obtain &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            \tilde{E}(\hat{f}) &amp;amp;= E(\hat{f}) + \eta^2
E^{R}(\hat{f}),
        
\end{aligned}\]&lt;/span&gt; where &lt;span
class=&quot;math display&quot;&gt;\[\begin{aligned}
            E^{R}(\hat{f}) &amp;amp;= \frac{1}{2} \iint \sum_k (\nabla
\hat{f}_k(x))^T (\nabla \hat{f}_k(x)) \cdot p(y_k | x) p(x) \text{ }dx
\text{ }dy_k\\
            &amp;amp;+ \frac{1}{4} \iint (y_k - \hat{f}_k(x))
\text{diag}(H_k) \cdot p(y_k | x) p(x) \text{ }dx \text{ }dy_k.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Keeping in mind that fully connected neural networks are compositions
of non-linear functions and affine transformations, it is easy to see
that this gives Tikhonov regularization.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;label-smoothing&quot;&gt;Label smoothing&lt;/h2&gt;
&lt;p&gt;Equivalent to adding noise to the output labels.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;learning-problem&quot;&gt;Learning problem&lt;/h2&gt;
&lt;p&gt;A learning problem is a triple &lt;span class=&quot;math inline&quot;&gt;\((T, E,
P)\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(T\)&lt;/span&gt; is the task,
&lt;span class=&quot;math inline&quot;&gt;\(E\)&lt;/span&gt; is experience, and &lt;span
class=&quot;math inline&quot;&gt;\(P\)&lt;/span&gt; is performance. For a specific task
&lt;span class=&quot;math inline&quot;&gt;\(T\)&lt;/span&gt;, we have a bundle of data points
&lt;span class=&quot;math inline&quot;&gt;\(E\)&lt;/span&gt; that will allow the model to
experience the task, and we evaluate the model’s performance using a
performance metric &lt;span class=&quot;math inline&quot;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Take the example of a student learning how to solve linear equations.
We might assign the student some homework problems, and later evaluate
the student based on how many questions he/she got correct out of 20
questions. Here, the learning problem is (solving linear equations,
homework assigments, test with 20 questions that is graded based on
proportion of correctly answered questions).&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;mini-batch-optimization&quot;&gt;Mini-batch
optimization&lt;/h2&gt;
&lt;p&gt;Mini-batch optimization uses only a subset of the training set for
each gradient step. For large datasets, mini-batch optimization is
preferred to its batch counterpart due to the computational demands of
batch optimization.&lt;/p&gt;
&lt;p&gt;A quick heuristic arguments in favor of mini-batch optimization: for
both batch and mini-batch optimization, the calculated gradient is an
averaged estimator for the true gradient. Given a batch of size &lt;span
class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;, the standard error is &lt;span
class=&quot;math inline&quot;&gt;\(\frac{\sigma}{\sqrt{n}}\)&lt;/span&gt;, where &lt;span
class=&quot;math inline&quot;&gt;\(\sigma\)&lt;/span&gt; is the true standard deviation.
While the size of the dataset grows linearly, the corresponding decrease
in standard error scales less than linearly - at some point, the extra
gains in variance reduction is not worth the compute.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;momentum&quot;&gt;Momentum&lt;/h2&gt;
&lt;p&gt;Momentum is an add-on to stochastic gradient descent to deal with the
noisy aspect of the stochastic gradient. The algorithm is as
follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose a learning rate &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;, a dampening rate &lt;span
class=&quot;math inline&quot;&gt;\(\alpha\)&lt;/span&gt;, initial momentum &lt;span
class=&quot;math inline&quot;&gt;\(v_0\)&lt;/span&gt;, and initial parameters &lt;span
class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;. Do the following until a
stopping criterion is met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take a mini-batch &lt;span class=&quot;math inline&quot;&gt;\(\{(x^{(i)},
y^{(i)})\}_{i=1,\cdots, m}\)&lt;/span&gt; from the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient estimate &lt;span
class=&quot;math inline&quot;&gt;\(\hat{g}_t = \frac{1}{m} \sum_{i=1}^m
\nabla_{\theta} L(f(x^{(i)}; \theta_t), y^{(i)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the momentum &lt;span class=&quot;math inline&quot;&gt;\(v_{t+1}
\leftarrow \alpha v_{t} - \epsilon \hat{g}_t\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the parameter estimate &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_{t} +
v_{t+1}.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Abusing notation just a bit, we can see that the new parameter
estimate is found through a decaying accumulation of past gradient
estimates: &lt;span class=&quot;math display&quot;&gt;\[\begin{aligned}
            \theta_{t+1} &amp;amp;= \theta_t + v_{t+1}\\
            &amp;amp;= \theta_t - \epsilon \hat{g}_t + \alpha v_t\\
            &amp;amp;= \theta_t - \epsilon \hat{g}_t + \alpha (-\epsilon
\hat{g}_{t-1} + \alpha v_{t-1})\\
            &amp;amp;\vdots\\
            &amp;amp;= \theta_t - \epsilon \left(\sum_{j=0}^{t} \alpha^{j}
\hat{g}_{t-j}  \right) + \alpha^{t+1} v_0.
        
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;nesterov-momentum&quot;&gt;Nesterov momentum&lt;/h2&gt;
&lt;p&gt;Nesterov momentum is an add-on to stochastic gradient descent with
momentum. The key difference is where the gradient is evaluated. The
algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose a learning rate &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;, a dampening rate &lt;span
class=&quot;math inline&quot;&gt;\(\alpha\)&lt;/span&gt;, initial momentum &lt;span
class=&quot;math inline&quot;&gt;\(v_0\)&lt;/span&gt;, and initial parameters &lt;span
class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;. Do the following until a
stopping criterion is met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take a mini-batch &lt;span class=&quot;math inline&quot;&gt;\(\{(x^{(i)},
y^{(i)})\}_{i=1,\cdots, m}\)&lt;/span&gt; from the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient estimate &lt;span
class=&quot;math display&quot;&gt;\[\hat{g}_t = \frac{1}{m} \sum_{i=1}^m
\nabla_{\theta} L(f(x^{(i)}; \theta_t + \alpha v_t),
y^{(i)})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the momentum &lt;span class=&quot;math inline&quot;&gt;\(v_{t+1}
\leftarrow \alpha v_{t} - \epsilon \hat{g}_t\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the parameter estimate &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_{t} +
v_{t+1}.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The gradient is evaluated closer to the next parameter update. Notice
that in SGD with momentum, the parameter update is &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow [\theta_t + \alpha v_t] -
\epsilon \hat{g}_t\)&lt;/span&gt;. With Nesterov momentum, the gradient is
evaluated with respect to &lt;span class=&quot;math inline&quot;&gt;\(\theta_t + \alpha
v_t\)&lt;/span&gt; - closer to where the parameter will eventually update
to.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;noise-robustness&quot;&gt;Noise Robustness&lt;/h2&gt;
&lt;p&gt;Small perturbations a.k.a. “noise” can be added to the input points,
the hidden layer units, the output labels, or the weights themselves.
This has a regularizing effect - one could even say that it makes the
network “antifragile”.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;rmsprop&quot;&gt;RMSProp&lt;/h2&gt;
&lt;p&gt;RMSProp belongs to a class of gradient descent algorithms with
adaptive learning rates. RMSProp is closely related to AdaGrad - instead
of aggregating the previous gradient estimates, RMSProp takes the
exponential weighted average. The RMSProp is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose a learning rate &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;, decay rate &lt;span
class=&quot;math inline&quot;&gt;\(\alpha \in [0,1)\)&lt;/span&gt;, and initial parameters
&lt;span class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;. Do the following until a
stopping criterion is met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take a mini-batch &lt;span class=&quot;math inline&quot;&gt;\(\{(x^{(i)},
y^{(i)})\}_{i=1,\cdots, m}\)&lt;/span&gt; from the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient estimate &lt;span class=&quot;math display&quot;&gt;\[g_t =
\frac{1}{m} \sum_{i=1}^m \nabla_{\theta} L(f(x^{(i)}; \theta_t),
y^{(i)})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aggregate and exponentially weight the outer product of all the
previous gradient estimates &lt;span class=&quot;math inline&quot;&gt;\(G_t = \alpha
G_{t-1} + (1 - \alpha) g_t g_t^T\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the parameter estimate &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_t - \epsilon
G_t^{-1/2} g_t.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Instead of using &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; to update
the parameters, it is common to use &lt;span
class=&quot;math inline&quot;&gt;\(\text{diag}(G_t + \delta I)\)&lt;/span&gt; with small
&lt;span class=&quot;math inline&quot;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt; to avoid singularity
and simplify computation; hence the update rule becomes &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_t - \epsilon \cdot
\text{diag}(G_t + \delta I)^{-1/2} g_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To see why this is an adaptive learning algorithm, observe &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon \cdot \text{diag}(G_t + \delta
I)^{-1/2}\)&lt;/span&gt;. The diagonal of &lt;span class=&quot;math inline&quot;&gt;\(G_t +
\delta I\)&lt;/span&gt; consists only of positive elements since &lt;span
class=&quot;math inline&quot;&gt;\(\alpha \in [0,1)\)&lt;/span&gt;, and each element
increases with each iteration. This means that the diagonal elements of
&lt;span class=&quot;math inline&quot;&gt;\(\text{diag}(G_t + \delta I)^{-1/2}\)&lt;/span&gt;
will shrink closer and closer to zero with each iteration - adapting the
learning rate.&lt;/p&gt;
&lt;p&gt;Notice that RMSProp is the same as AdaGrad except that exponential
weighting is applied to the gradient accumulation.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;stochastic-gradient-descent-sgd&quot;&gt;Stochastic
gradient descent (SGD)&lt;/h2&gt;
&lt;p&gt;The stochastic gradient descent algorithm (SGD) is a mini-batch
optimization algorithm. The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose a learning rate &lt;span
class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt; and initial parameters &lt;span
class=&quot;math inline&quot;&gt;\(\theta_0\)&lt;/span&gt;. Do the following until a
stopping criterion is met (e.g. early stopping):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Take a mini-batch &lt;span class=&quot;math inline&quot;&gt;\(\{(x^{(i)},
y^{(i)})\}_{i=1,\cdots, m}\)&lt;/span&gt; from the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the gradient estimate &lt;span
class=&quot;math inline&quot;&gt;\(\hat{g}_t = \frac{1}{m} \sum_{i=1}^m
\nabla_{\theta} L(f(x^{(i)}; \theta_t), y^{(i)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update the parameter estimate &lt;span
class=&quot;math inline&quot;&gt;\(\theta_{t+1} \leftarrow \theta_{t} - \epsilon
\hat{g}_t.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SGD can be too noisy and/or unstable, and finding a good learning
rate is critical. Momentum and adaptive learning algorithms solve the
former and latter issues, respectively.&lt;/p&gt;
&lt;h2 class=&quot;unnumbered&quot; id=&quot;validation-set&quot;&gt;Validation set&lt;/h2&gt;
&lt;p&gt;The validation set is used to decide which model to use. The
validation set is usually used in tandem with the training set - at the
end of every training epoch, evaluate the trained model on a
never-before-seen subset of the validation set. To evaluate the model,
use the original loss instead of the surrogate loss used during
training.&lt;/p&gt;</content><author><name></name></author><summary type="html">AdaGrad | Adam | Batch Optimization | Bias | Bias-variance decomposition | Bootstrap aggregating | Dropout | Early stopping | Empirical risk minimization | Gradient | Gradient descent | Hessian | L1 Regularization | L2 Regularization | Label smoothing | Learning problem | Mini-batch optimization | Momentum | Nesterov momentum | Noise Robustness | RMSProp | Stochastic gradient descent (SGD) | Validation set |</summary></entry><entry><title type="html">Paradoxical Thinking</title><link href="http://localhost:3000/2022/02/25/contradictions/" rel="alternate" type="text/html" title="Paradoxical Thinking" /><published>2022-02-25T00:00:00-06:00</published><updated>2022-02-25T00:00:00-06:00</updated><id>http://localhost:3000/2022/02/25/contradictions</id><content type="html" xml:base="http://localhost:3000/2022/02/25/contradictions/">&lt;div class=&quot;marginnote&quot;&gt;&lt;/div&gt;

&lt;p&gt;Since high school, I&apos;ve had an obsession with contradictions and paradoxes. If I had to pinpoint any one thing to think about for the rest of time, it&apos;d be intersections of contradictions. &lt;div class=&quot;marginnote&quot;&gt;I once asked my dad in high school how I was supposed to be confident and humble at the same time. He said: &quot;You&apos;ll figure it out.&quot; I still haven&apos;t figured it out.&lt;/div&gt;&lt;/p&gt; 

&lt;p&gt;From a purely logical standpoint, contradictions shouldn&apos;t exist. Yet we hear them all the time, summarized in captivating symbols or short aphorisms: &quot;empty space is what makes a bowl useful&quot;, &quot;be strong yet weak&quot;, etc.&lt;/p&gt;

&lt;p&gt;Why are they so ubiquitous, in spite of their impossibility? Being a math person, this question plagued me, until I figured out how to reconcile some types of contradictions. What I show below is more a useful analytical framework for understanding contradictory phenomena than a concrete analysis of complexity or contradictions.&lt;/p&gt;

&lt;h3&gt;Frames of reference&lt;/h3&gt;

&lt;p&gt;Many types of contradictions can be resolved by looking at different frames of reference (FOR). There may be other types of contradictions that don&apos;t buckle under the &quot;FOR attack&quot; but I&apos;ve found this idea to be helpful for pinpointing what causes contradictory behavior.&lt;/p&gt; 

&lt;p&gt;An object can simultaneously exhibit one property at one FOR but the opposite property at another FOR. The key insight is that descriptions apply only for specific FORs.&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;marginnote&quot;&gt;Most misunderstandings arise from a difference in FOR. Recently, I became incredibly frustrated during a philosophy of biology class where the professor claimed that choosing a mate is comparable to eugenics. &apos;Eugenics&apos;, as most know, is a socially, culturally, and morally loaded word. It seems obvious (but apparently not obvious enough) that eugenics applies to a much larger FOR than sexual selection does, so to say that the two are comparable is just sloppy.&lt;/div&gt;I can call Chicago and New York &quot;close&quot; when looking at a map, but when I ditch the map and go walking from Chicago to New York, Chicago and New York are &quot;far apart&quot;. This means that when describing something, we need to ensure that we include which FOR we are in. &lt;/p&gt;

&lt;h4&gt;Size variation&lt;/h4&gt;

&lt;p&gt;My favorite method of changing FORs is altering the size of the frame window a.k.a. size variation. This is an obvious method - of course things look different when you look at them closely vs. at a distance!&lt;/p&gt; 

&lt;img src=&quot;/assets/data/paradox/size.png&quot; style=&quot;display: block; width: 90%;margin-left: auto; margin-right: auto;&quot;&gt;

&lt;p&gt;&lt;div class=&quot;marginnote&quot;&gt;I wrap &apos;both&apos; in quotation marks because strictly speaking, a circle can&apos;t be linear by definition.&lt;/div&gt;Although deceivingly simple and almost stupid, this idea isn&apos;t completely useless. For example, a circle is circular when viewed through a large frame window, but the circle &quot;straightens out&quot; into a line at a sufficiently small frame window. In this way, a circle is &quot;both&quot; linear and circular.&lt;/p&gt;


&lt;p&gt;In fact, a curved object exhibiting both circular and linear properties by way of varying FOR size is the inspiration for using derivatives to approximate differentiable curves. Generally, linearity lends itself to easier computation, so having a linear approximation for a complex, bendy curve can be incredibly useful (given a reasonable error margin, of course). &lt;/p&gt;

&lt;h4&gt;Positive-negative variation&lt;/h4&gt;

&lt;p&gt;Another useful variation is the positive-negative variation. &lt;/p&gt;

&lt;p&gt;The name of this variation derives from the artistic notion of positive and negative space. The positive space of the &lt;i&gt;Mona Lisa&lt;/i&gt; consists of the Mona Lisa herself, while the negative space is the faded trees and hills in the background. &lt;/p&gt;

&lt;img src=&quot;/assets/data/paradox/mona.png&quot; style=&quot;display: block; width: 90%;margin-left: auto; margin-right: auto;&quot;&gt;

&lt;p&gt;To apply positive-negative variation, break an object or concept down into its consituent parts. As an example, consider societies. The constituent parts (i.e. individuals) of a society come together and form &lt;i&gt;relationships&lt;/i&gt; in order to form the whole. The &quot;positive space&quot; of the society then consists of its individuals, while the &quot;negative space&quot; consists of the relationships between them.&lt;/p&gt;

&lt;img src=&quot;/assets/data/paradox/graph.png&quot; style=&quot;display: block; width: 90%;margin-left: auto; margin-right: auto;&quot;&gt;

&lt;p&gt;Positive-negative variation is easily visualized by taking any network and breaking it down into its nodes and edges. The positive space is just the nodes without the edges, and likewise the negative space is the edges without the nodes.From a positive FOR, the properties of the nodes become visible. The negative FOR reveals how nodes behave when they clash with other nodes. &lt;div class=&quot;marginnote&quot;&gt;I&apos;ve always said it and I&apos;ll say it again: &quot;people are simple, but it&apos;s only when people come into relationships that things get complicated.&quot;&lt;/div&gt;&lt;/p&gt;



&lt;h4&gt;Relative-absolute variation&lt;/h4&gt;

&lt;p&gt;You can put different variations together to get new flavors. The relative-absolute variation is one such variation - it is the combination of size and positive-negative variation in that it is positive-negative variation viewed on a micro-scale. It addresses the question &quot;How does the &lt;b&gt;essence&lt;/b&gt; of an object affect the nature of its &lt;b&gt;existence&lt;/b&gt;, and vice versa?&quot;.&lt;/p&gt; 

&lt;p&gt;The insight for relative-absolute variation is that, except for things that live in complete isolation, most objects or concepts have a relative and an absolute definition. The former defines by way of relationships to other things, whereas the latter defines the thing in its own terms. &lt;/p&gt;

&lt;p&gt;A relative definition is dependent on other things, while an absolute definition is independent and can stand on its own. For example, the relative definitions of me would be &quot;first son of - and -, brother of -, etc.&quot;, while my absolute definition would be &quot;homo sapien&quot;.&lt;/p&gt;  

&lt;p&gt;&lt;div class=&quot;marginnote&quot;&gt;It is possible for relative definitions to imply absolute definitions, hence making relative and absolute equivalent. Refer to Case Study 1 for the canonical example.&lt;/div&gt;Notice that the absolute definitions automatically imply any relative definitions, if and when the absolute definitions exist. Wouldn&apos;t it be nice if we had absolute definitions for every object and concept, then? Sadly, absolute definitions are hard to find in non-abstract worlds, so relative definitions are usually all we can work with.&lt;/p&gt; 

&lt;h4&gt;Caveat: cross-contaminated FORs&lt;/h4&gt;

&lt;p&gt;Sometimes, descriptions carry over into other FORs. The tricky bit with relative-absolute variation is that the absolute definition implies the relative defintions.&lt;/p&gt;


&lt;h4&gt;Example&lt;/h4&gt;

&lt;p&gt;Sequences are a perfect demonstration of the relative-absolute variation at work.&lt;/p&gt;

&lt;p&gt;Assume that a generic metric space \( (E, \rho) \).&lt;/p&gt; 

&lt;p&gt;\(\text{Definition: }\) A sequence \( \{a_n\} \) converges to \(a \in E \) if for all \( \epsilon &gt; 0 \), there exists \( N \) such that if \( n \geq N \), then \( \rho(a_n, a) &lt; \epsilon \).&lt;/p&gt;

&lt;p&gt;\(\text{Definition: }\) A sequence \( \{a_n\} \) is a &lt;i&gt;Cauchy sequence&lt;/i&gt; if for all \(\epsilon &gt; 0\), there exists \(N\) such that if \(m, n \geq N\), then \( \rho(a_m, a_n) &lt; \epsilon \).&lt;/p&gt;

&lt;p&gt;Notice that a Cauchy sequence gives a relative definition: the convergent-esque behavior of a sequence is determined by the distances between the elements past a certain threshold. On the other hand, the definition of convergence is absolute.&lt;/p&gt; 

&lt;p&gt;In all cases, convergence implies Cauchy-ness. The converse is not always true, however. This isn&apos;t intuitive, as you&apos;d expect elements that get closer and closer to each other to converge to &lt;i&gt;something&lt;/i&gt;. Finding a condition for which Cauchy-ness implies convergence is then a non-trivial task.&lt;/p&gt; 

&lt;p&gt;Luckily, we know what that condition is. The spaces where the converse holds are called &lt;i&gt;complete metric spaces&lt;/i&gt;. In complete metric spaces, Cauchy sequences and convergent sequences are equivalent; the relative definition equals the absolute definition.&lt;/p&gt;

&lt;p&gt;This is highly unusual. It&apos;s very rare for relative and absolute definitions to match up. Things like complete metric spaces don&apos;t pop up often in the natural/social world, so I haven&apos;t found a social analogue for complete metric spaces yet.&lt;/p&gt; 

&lt;!-- &lt;h4&gt;Case study 1: Fractals&lt;/h4&gt;

&lt;p&gt;I&apos;ll say it: fractals are the purest, most beautiful contradictions. Many fractals exhibit non-sensical properties: Sierpinski&apos;s triangle has zero area, the Cantor middle-thirds set is uncountably infinite but has length zero. &lt;/p&gt;

&lt;p&gt;They are both simple and complex - simple in that any fractal can be constructed by repeating the rule of self-similarity across all scales and locations. Complex in that this simple rule creates unimaginable detail. Somehow, the fractal is more than the sum of its parts.&lt;/p&gt; 

&lt;p&gt;Fractals can be analyzed using size variation and positive-negative variation.&lt;/p&gt;

&lt;p&gt;Size variation analysis isn&apos;t too fruitful - the main observation is that fractals look largely the same regardless of the viewing window size.&lt;/p&gt; 

&lt;p&gt;Positive-negative variation, on the other hand, gives good support for why nature is deceptively simple but amazingly complex. &lt;/p&gt;

&lt;h4&gt;Case study 2: Society and power&lt;/h4&gt;

&lt;p&gt;&lt;div class=&quot;marginnote&quot;&gt;Think about every bad party you&apos;ve been to. There is no &quot;energy&quot;. Compare this to a lively party, where everyone is in sync - Durkheim called this force &quot;collective effervescence&quot;.&lt;/div&gt; Durkheim famously said that society is &lt;i&gt;sui generis&lt;/i&gt; because there is something else, some indescribable social force that emerges from a truly coalesced collective. Marx remarked in &lt;i&gt;The Grundrisse&lt;/i&gt; that &quot;society does not consist of individuals, but expresses the sum of interrelations, the relations within which these individuals stand.&quot;&lt;/p&gt; 

&lt;p&gt;Durkheim and Marx&apos;s insights point towards an analysis using positive-negative variation. The representative diagram of a society is the social graph: the nodes being actors and the edges being the relationships. We extract that the edges are just as important as the nodes.&lt;/p&gt; 

&lt;p&gt;Let&apos;s add another degree of complexity. The picture is further complicated when power comes into play - not all relationships are created equal. There exists an edge connecting a king and his vassal, but surely the king can exercise greater power over his vassal than the other way around.&lt;/p&gt; 

&lt;p&gt;The superimposition of the power graph onto the existing social graph leads to perhaps the greatest contradiction of social engineering of the 20th century.&lt;/p&gt; 

&lt;h4&gt;Case study 3: Market dynamics&lt;/h4&gt;

&lt;p&gt;In the casino, some are playing long game, quietly gathering profits and taking safer bets. Others are in it for the rush, looking for the bet that&apos;ll fill their hands with cash. Using the language of size variation, the former and latter are operating on larger and smaller windows, respectively. &lt;/p&gt;

&lt;p&gt;Given that markets are basically casinos, let me first be clear in saying that there is no contradiction here - these two types of players will rarely collide in terms of ethos or strategy. As Benjamin Graham famously described, &quot;in the short run, the market is a voting machine but in the long run, it&apos;s a weighing machine&quot;. Assuming that his thesis is correct, the short-run players are primarily betting on social value, whereas long-run players are betting on real value. Although they rarely collide, they do interact with each other in the sense that both are participating in the ebbs and flows of the market - this indicates that positive-negative variation may be useful.&lt;/p&gt;

&lt;p&gt;Remembering that relative-absolute variation is a combination of size variation and positive-negative variation, this should send off alarm bells. Are short-run and long-run players operating respectively on relative and absolute definitions?&lt;/p&gt; 

&lt;p&gt;The main question that must be addresssed for us to prove our theory true is if real vaue implies social value. Given that we are operating in the social sphere, we can be lax with how &quot;strong&quot; the implication needs to be.&lt;/p&gt;

&lt;p&gt;But what happens when these players begin to collide with each other? What if the short-run players suddenly decide to play the long game, vice versa, or both types of players meet in the middle? Here, definitions of value conflate. And it is here that we find the social analogue of the complete metric space.&lt;/p&gt;  


&lt;h5&gt;Appendix&lt;/h5&gt;

\(f(x) = f(a) + f&apos;(a)(x-a) + \frac{f&apos;&apos;(t)}{2!}(x-a)^2\). 
 --&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">2021 in Review</title><link href="http://localhost:3000/2021/12/24/2021letter/" rel="alternate" type="text/html" title="2021 in Review" /><published>2021-12-24T00:00:00-06:00</published><updated>2021-12-24T00:00:00-06:00</updated><id>http://localhost:3000/2021/12/24/2021letter</id><content type="html" xml:base="http://localhost:3000/2021/12/24/2021letter/">&lt;p&gt;My friend &lt;a href=&quot;https://tkoay.wordpress.com/&quot;&gt;Tim&lt;/a&gt;, among others like Dan Wang, writes an annual letter. I decided that I should do the same, since I&apos;m just a high-stressed munchkin that never looks backwards.&lt;/p&gt;

&lt;h2&gt;...&lt;/h2&gt;

&lt;div class=&quot;marginnote&quot;&gt;&lt;/div&gt;

&lt;p&gt;What a whirlwind it has been these past 12 months.&lt;/p&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[1] 有朋自遠方來 不亦樂乎: To have friends come from distant quarters; is this not a pleasure?&lt;/div&gt;&lt;div class=&quot;marginnote&quot;&gt;天下大勢 分久必合 合久必分: The empire, long divided, must unite; long united, must divide.&lt;/div&gt;

&lt;p&gt;2021 was a year of new beginnings and reincarnated challenges. It was the first full calendar year that I spent outside of Korean military service, and the first time that I stepped back onto American soil since 2018. It was a year of seeing old faces (in person!), mending worn relationships, and igniting new friendships. [1]&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;[2] I&apos;m not saying that there will ever be a return to &apos;normalcy&apos;, whatever that means. We are riding the wave of the new normal - but this doesn&apos;t mean that COVID will always pose as much of a threat as it does now.&lt;/div&gt;

&lt;p&gt;COVID continues to ravage the world since it first struck in the winter of 2019, and the emotional rollercoaster doesn&apos;t seem to be ending anytime soon. During the summer, we rejoiced and placed our faith in vaccines to stop COVID dead in its tracks - but the colder seasons smothered that faith, with COVID variants such as Delta and Omicron continuing to emerge. Still, like the young David&apos;s message to Goliath, we tell COVID: &quot;the Lord will deliver you into my hand, and I will strike you down.&quot; [2]&lt;/p&gt;



&lt;h3&gt;Personal&lt;/h3&gt;

&lt;p&gt;&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;&lt;img src=&quot;/assets/img/plato-aristotle.jpeg&quot; style=&quot;width: 40%; &quot;&gt;&lt;/div&gt; 
&lt;div class=&quot;marginnote&quot;&gt;[3] At last, I&apos;m becoming more like Aristotle and less like that bastard Plato.&lt;/div&gt;

&lt;p&gt;Having been a math and philosophy person for my entire life, I resolved to focus less on the abstract and more on the concrete. [3] I read the &lt;a href=&quot;https://alikiki.github.io/2021/06/30/news/&quot;&gt;news&lt;/a&gt; more often and explored personally unfamiliar fields: economics, anthropology, sociology, business, finance, history, etc. I&apos;m pleased to report that my efforts are not going to waste - I&apos;m able to examine ideas in more diverse perspectives, and engage in conversations that I previously had no idea &lt;i&gt;what&lt;/i&gt; to think. There&apos;s still so, so much to learn, but I&apos;m excited to see how my thought process changes throughout the rest... of my lifetime.&lt;/p&gt;

&lt;p&gt;Paradoxes continue to be the framework on which my thought processes are based. I live for the absolute euphoria that comes from making cross-disciplinary connections via paradoxical thinking.&lt;/p&gt;

&lt;p&gt;I remain scattered in my interests. This year, I contemplated taking up visual art, architecture, stand-up comedy, and a smattering of others. Despite my disorganized half-ambitions, I am still guided by the same essential questions:
&lt;ol&gt;
	&lt;li&gt;How can things grow &apos;organically&apos;, and which forces must be applied or not applied?&lt;/li&gt;
	&lt;li&gt;What drives the naturalistic fallacy?&lt;/li&gt;
	&lt;li&gt;Why do we/I gravitate towards dichomoties? What consequences does this have for contradiction-based thinking?&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;[4] Yes, I am Asian. No, not every Asian&apos;s eyes disappear when smiling.&lt;/div&gt;
&lt;p&gt;It embarrasses me to admit it, but one of the most memorable moments of the year was seeing my grandmother smile from cheek-to-cheek. I unfortunately have a bad habit of not looking elders in the eye, but that one twinkle of her smile cured me. I was surprised by how similar her smile was to mine - the same disappearing of the eyes [4], the same curvature of the lips, etc. I&apos;ve always felt a little afraid (?) around my grandparents, but her smile reminded me that I need to make a better effort to understand where I come from. They were once my age, anyways.&lt;/p&gt;

&lt;p&gt;I&apos;m thankful for COVID for bringing my family some much-needed time together. This past year was the longest stretch of time that I spent at home since I left in ninth grade. It was no surprise that it took some time to adjust. I&apos;d changed a lot as a person from 2013 to 2021, so the lenses through which my family viewed me and vice versa were frozen in time. Regardless, family is family, so the transition happened with relatively few bumpy bits along the way.&lt;/p&gt;

&lt;h3&gt;Thoughts&lt;/h3&gt;

&lt;h4&gt;Standardization&lt;/h4&gt;
&lt;p&gt;Ever since I read James C. Scott&apos;s &lt;i&gt;Seeing Like A State&lt;/i&gt;, legibility has been on my mind. &lt;/p&gt;

&lt;p&gt;There&apos;s no question that there exists a contradiction between legibility/standardization and diversity. Both have their pros and cons. In fact, diversity excels where legibility fails, and vice versa.&lt;/p&gt;

&lt;p&gt;Legibility is useful in convenience-first contexts, since it eliminates the need to convert between different forms. However, standardized forms are one-size-fits-all by definition, which means that there exists a limit to how effectively they can address a problem domain.&lt;/p&gt;

&lt;p&gt;In contrast, this is where diversity excels. Specialized tools, when solving the problems for which they are built, will always outperform general-purpose tools. Nonetheless, diversity necessitates that extra step of conversion.&lt;/p&gt;

&lt;p&gt;When power comes into the picture, things get more complicated. In particular, installing a standardized system requires a force that&apos;s able to impose its strength over &lt;i&gt;all&lt;/i&gt; other forces i.e. &lt;i&gt;a centralizing force&lt;/i&gt;. Diversity doesn&apos;t call for a power like this.&lt;/p&gt;

&lt;p&gt;Suddenly, standardization feels dangerous. Having to be subservient to a centralizing power in exchange for the comfort of not having to convert between different forms? Most people would say no.&lt;/p&gt;

&lt;p&gt;But wait! This tension has reared its head over and over again throughout history, so we shouldn&apos;t be so quick to judge. Consider the following examples/scenarios:&lt;/p&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[5] These fees are called interchange fees or swipe fees, because they are charged at every swipe.&lt;/div&gt;
	&lt;li&gt;&lt;i&gt;Credit cards.&lt;/i&gt; Before credit cards came around, merchants had to maintain credit accounts for every individual customer. This meant a lot of number crunching and headaches. Bank of America then charged onto the scene and launched BankAmericard in 1958. They proposed the following deal: we will take on all the mind-numbing maintenance work if you pay us a fee. [5] By relinquishing control over maintaining their customers&apos; credit accounts, merchants gained access to more potential customers and less headaches.&lt;/li&gt;

	&lt;p&gt;&lt;/p&gt;

	&lt;li&gt;&lt;i&gt;Shipping containers.&lt;/i&gt; When a business/person operates on a specific section of the value chain, they will not want to give up control. Such is the story of the modern shipping container: countless rate wars, labor union strikes, private vs. public interest, etc. Before Malcolm McLean entered the picture, containers were already demonstrated to be economically superior. The main objection arose from the fact that reducing friction via standardization would mean momentarily less profits for each component on the value chain. Nonetheless, most of us would probably agree that containerization has made supply chains and global trade much more efficient.&lt;/li&gt;

	&lt;p&gt;&lt;/p&gt;

	&lt;li&gt;&lt;i&gt;A world of abundance.&lt;/i&gt; In a world of abundance, the need for convenience imposes a centralizing force. Consider search as a centralizer. Search is such a powerful product because its utility scales with every added bit of information, and the execution of search itself adds information. With 10mb, search is more for shallow convenience than anything else; but with 10 exabytes, it becomes a necessity. &lt;a href=&quot;https://ourworldindata.org/grapher/share-of-individuals-using-the-internet?tab=chart&quot;&gt;More and more people are using the Internet everyday&lt;/a&gt;, and the information is being added at an exponential rate. A centralizer like search allows users to access relevant information more quickly and accurately.  Ben Thompson&apos;s &lt;a href=&quot;https://stratechery.com/2015/aggregation-theory/&quot;&gt;Aggregation Theory&lt;/a&gt; posits that companies that control demand for abundant resources win out against those that control the distribution of scarce resources - and search is no exception.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;marginnote&quot;&gt;[6] &lt;a href=&quot;https://moxie.org/2022/01/07/web3-first-impressions.html&quot;&gt;This essay&lt;/a&gt; by Moxie is a good example of what I mean.&lt;/div&gt;

&lt;p&gt;This isn&apos;t to say that centralization is a silver bullet to all problems. The answer is, of course, context-dependent. All I&apos;m trying to demonstrate is that centralizing isn&apos;t a bad thing all the time. The canonical Spiderman aphorism - &quot;with great power comes great responsibility&quot; - rings true. [6] Having your own power is great, until the maintenance fees come due.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h4&gt;Naturalistic fallacy&lt;/h4&gt;
&lt;p&gt;The naturalistic fallacy is when we assume that natural implies good. Philosophers generally call this &quot;deriving an ought from an is&quot;. &lt;/p&gt;

&lt;p&gt;I&apos;ve been wondering why the naturalistic fallacy exists, and I think it has something to do with our current understanding of biology.&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;[7] I&apos;m not saying that biology is completely separable from chemistry or physics. Physics and chemistry can both be applied to biology, but only in the smaller scales. I&apos;m talking about biology in its conventional large scale.&lt;/div&gt;

&lt;p&gt;Like math, biology occupies a weird place among the sciences. It seems to be the only science that can&apos;t be explained mechanistically. In contrast, we talk about chemical reactions as a function of molecular properties - not as a function of what the reaction is supposed to achieve. For physics also, we say &quot;electrons are attracted to protons&quot; instead of &quot;electrons are attracted to protons so that something else may occur&quot;. [7]&lt;/p&gt;

&lt;p&gt;But in biology, we&apos;re taught that a bird&apos;s beak is shaped &lt;i&gt;so that&lt;/i&gt; it can crack open nuts better. We&apos;re taught that chameleons can change color &lt;i&gt; so that &lt;/i&gt; it can hide better. See the difference?&lt;/p&gt;

&lt;p&gt;We understand biology as a &lt;i&gt;teleological&lt;/i&gt; science. But is there really any other way to understand it? Without teleology, how can you explain nature&apos;s extraordinary regularity or its magnificent diversity?&lt;/p&gt;

&lt;p&gt;You&apos;ll undoubtedly shove the central dogma of biology in my face. But DNA, RNA, or proteins can&apos;t explain seemingly perfect adaptations - opposable thumbs, camouflage, a woodpecker&apos;s skull, etc. In the current theory, the central dogma is akin to a computer following instructions. But where did these instructions come from, and why are they written in the way that they are?&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;[8] The original &lt;i&gt;On the Origin of Species&lt;/i&gt; actually contains a lot of teleological language. For example, natural selection acts as an invisible guide, directing organisms towards the &quot;good&quot;.&lt;/div&gt;
&lt;p&gt;Again, you&apos;ll object by gesturing towards natural selection. [8] Although powerful, natural selection actually relies on teleology. We say that an adaptation formed because it was advantageous to its possessors. But our definition of &quot;advantageous&quot; depends on how we understand the adaptation &lt;i&gt;currently&lt;/i&gt;. In essence, we&apos;re retro-projecting - we infer a &lt;i&gt;past&lt;/i&gt; event based on the &lt;i&gt;present&lt;/i&gt; state.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Biology can&apos;t seem to escape from teleology;&lt;/b&gt; the world seems to have been designed by an omniscient designer who uses natural selection as his primary tool. &lt;/p&gt;

&lt;p&gt;The naturalistic fallacy is so potent because we understand biology as teleological. An all-knowing master designer, creating life that is so perfectly well-adapted... how could this master designer be evil? Surely what this designer has built must be more &quot;good&quot; than any of our creations?&lt;/p&gt;

&lt;p&gt;This means that the naturalistic fallacy has everything to do with religion, but in a different form. Post-Kuhn, we often call science a religion, and biology is the canonical science-religion because it is teleological. &lt;/p&gt;


&lt;h4&gt;Machine Learning [9]&lt;/h4&gt;

&lt;div class=&quot;marginnote&quot;&gt;[9] I refer to machine learning and artificial intelligence interchangeably. They&apos;re the same thing, anyway.&lt;/div&gt;

&lt;p&gt;In &lt;i&gt;Seeing Like A State&lt;/i&gt;, Scott claims that a state can only see legible things. Therefore, it must spend most of its early days trying to make illegible things legible.&lt;/p&gt;

&lt;p&gt;I originally thought that AI&apos;s main strength was in legible-izing any illegible construct. With enough data, we can train any model to detect patterns or recognize a form. AI is an &lt;i&gt;adaptable&lt;/i&gt; tool that&apos;ll better preserve &quot;local&quot; knowledge while still being able to extract useful information i.e. legible-ize local information.&lt;/p&gt;

&lt;p&gt;While this may remain a core strength, I remembered the fundamental question of any technology: &lt;i&gt;does it free up time and cognitive resources?&lt;/i&gt; There&apos;s a reason every technological boost appears to have socioeconomic implications.&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;[10] Child employment as a measurement of cognitive demand feels dangerous. But I wonder if it actually can be a metric, since it seems to signify de-skilling.&lt;/div&gt;

&lt;p&gt;Invented during the first Industrial Revolution, the power loom saved both time and cognitive resources. Time - because the loom was much faster than human weavers. Cognitive resources - because its speed reduced the demand for skilled weavers, and child employment in power loom mills rose. [10]&lt;/p&gt;

&lt;p&gt;Another canonical example of technology: computing devices e.g. the abacus, the logarithmic slide rule, mechanical calculators, electronic calculators, computers etc. All reduce time and cognitive strain.&lt;/p&gt;


&lt;p&gt;All of this is to say: machine learning&apos;s main strength is in freeing up time and cognitive resources, &lt;i&gt;so that&lt;/i&gt; people can better focus on the issues that matter. Sure, deep learning techniques continue to produce &lt;a href=&quot;https://openai.com/blog/instruction-following/&quot;&gt;stunning results by the day&lt;/a&gt;. But any startup that claims that their artificial intelligence is the &quot;future&quot; is missing the point. It&apos;s the future in the sense that that&apos;s how mundane tasks will be completed - but it&apos;s not how future solutions to future problems will look.&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;[11] This follows intuitively and mathematically. Look at any limiting theorem of probability e.g. the central limit theorem, law of large numbers, Chernoff bounds, etc.&lt;/div&gt;

&lt;p&gt;Let me provide an example. The chief pitfall of machine learning is that you need a lot of data. The more data you have, the better your model will be. [11] However, &quot;ground-truth&quot; data will always be unbalanced - your model will predict everyday, mundane events far better than black swan events because your data is inherently skewed. There are techniques for dealing with unbalanced data, but let&apos;s face it: smart techniques are for the resource-poor, and they will never yield comparatively better results.&lt;/p&gt;

&lt;p&gt;But that&apos;s okay. Leave the mundane tasks to the computers, and let the humans work at the more important problems. I don&apos;t know what future solutions to future problems will look like, but human contributions will definitely be present.&lt;/p&gt;


&lt;h3&gt;Going Forward&lt;/h3&gt;
&lt;p&gt;I don&apos;t usually form New Year&apos;s resolutions and I don&apos;t intend to start now. My listing these &quot;goals&quot; here is more for accountability than anything else.&lt;/p&gt;

&lt;h4&gt;To reflect&lt;/h4&gt;
&lt;p&gt;This year, I aim to spend more time in reflection. I&apos;m very high strung, and anyone who has lived with me will tell you that I&apos;m constantly stressed. I flipped through my journal entries for the year and discovered that almost every single entry was about needing to be better!&lt;/p&gt;

&lt;p&gt;That&apos;s not great. Something needs to change, clearly.&lt;/p&gt;

&lt;p&gt;Storytime. In the military, every soldier goes through five weeks of basic training. Once education is complete, soldiers are allowed five hours with their families before getting assigned to their bases. For me, the five weeks of training felt like an eternity, but the five hours of bliss disappeared in a &lt;i&gt;*snap*&lt;/i&gt;. As I walked back to the camp, thinking of the hell ahead, my father told me, &quot;You need to look forwards and backwards - forwards to motivate you, backwards to know you&apos;re going in the right direction.&quot;&lt;/p&gt;

&lt;p&gt;This leads to a more general piece of advice for my future self: &lt;i&gt;When regretful, look forwards. When anxious, look backwards. When rushed, look far. When overwhelmed, look close.&lt;/i&gt;&lt;/p&gt;

&lt;h4&gt;To form verifiable opinions&lt;/h4&gt;
&lt;p&gt;For most of my life, I&apos;ve seeked to understand all sides but was content to stop there - all the while forgetting to form an opinion myself. Common-sense dictates that this isn&apos;t great in the long term.&lt;/p&gt;

&lt;p&gt;In poker, it&apos;s &lt;i&gt;usually&lt;/i&gt; better to fold or raise than to call. A random walk with unequal probabilities of going left/right will most definitely travel further than a simple random walk will.&lt;/p&gt;

&lt;p&gt;At the same time, though, it&apos;s critical to recognize that directionality isn&apos;t always a winning play. Sometimes, the optimal strategy is the opposite: go forth and diversify. But, for the time being, I need to form opinions and justify/update them.&lt;/p&gt;

&lt;p&gt;In particular, my opinions must be verifiable! I usually gravitate towards more philosophical questions, and philosophical questions aren&apos;t usually verifiable. Hence, not only must I form actual opinions, I must form opinions on questions that are empirically or logically &lt;i&gt;verifiable&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;I look forward to the coming year. See you all soon.&lt;/p&gt;</content><author><name></name></author><summary type="html">My friend Tim, among others like Dan Wang, writes an annual letter. I decided that I should do the same, since I&apos;m just a high-stressed munchkin that never looks backwards.</summary></entry><entry><title type="html">Today in News</title><link href="http://localhost:3000/2021/06/30/news/" rel="alternate" type="text/html" title="Today in News" /><published>2021-06-30T00:00:00-05:00</published><updated>2021-06-30T00:00:00-05:00</updated><id>http://localhost:3000/2021/06/30/news</id><content type="html" xml:base="http://localhost:3000/2021/06/30/news/">&lt;h2&gt;[Completed.]&lt;/h2&gt;

&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;Louis Pasteur said &quot;where observation is concerned, chance favors only the prepared mind.&quot; For me, there are three components to intellectual growth: observation, intake, and reflection. Observation is by far the component that I pay the least attention to, and this page is an attempt to give more effort to observation.&lt;/p&gt;

&lt;p&gt;In other words, I&apos;m just trying to trick myself into not being a theoretical-head-in-the-clouds geek. Mind you, the news articles listed here are not the most important or interesting, but more a reflection of what I am able to address.&lt;/p&gt;

&lt;div class=&quot;marginnote&quot;&gt;&lt;/div&gt;

&lt;h3&gt;Thursday, September 2, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;Back in February, Shell acquired Ubitricity, a company that develops EV charging infrastructure.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/technology/shell-vastly-expand-ev-charging-network-britain-2021-08-31/&quot;&gt;Royal Dutch Shell&lt;/a&gt; aims to increase the number of electric vehicle (EV) charging points around Great Britain. This expansion is in line with the country&apos;s plans to ban the sale of new petrol and diesel cars by 2030, and to go net zero by 2050.[1]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/energy-traders-see-big-money-in-carbon-emissions-markets-11630488780&quot;&gt;Carbon-emissions markets&lt;/a&gt; heating up, making energy traders perk up. With emerging carbon-emissions market in China and the expanding market in Europe, trading houses look optimistically at growth potential.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Tuesday, August 24, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.eia.gov/todayinenergy/detail.php?id=49236&quot;&gt;U.S. battery storage capacity grew 35% in 2020.&lt;/a&gt;The fast-increasing trend is expected to continue. [1]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/d673c756-eb4c-45e1-822d-b520158768c6&quot;&gt;Pfizer to buy Trillium Therapeutics&lt;/a&gt;. Trillium is an oncology biotech company that has two promising drugs that harness the immune system to target blood cancers such as leukemia. Pfizer&apos;s oncology division seems to be betting on the success of these drugs in early to mid-stage trials.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Thursday, August 12, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/a8a631cf-de43-47e8-8cc4-99732c39c4da&quot;&gt;Biden administration&lt;/a&gt; wants OPEC to raise oil production to counter rising U.S. petrol prices. Despite OPEC+ agreeing to boost supply last month, petrol prices have risen. The FTC was also urged to crack down on any collusions in the petrol market.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-08-11/hackers-return-funds-from-likely-record-defi-crypto-attack&quot;&gt;Hackers&lt;/a&gt; return stolen funds to Poly Network.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/a778224b-83d4-4679-8ebf-5bdb80e577a3&quot;&gt;Tesla demand&lt;/a&gt; falls in China while demand for domestic EV makers surges.&lt;/li&gt;

&lt;/ol&gt;

&lt;h3&gt;Wednesday, August 11, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] Many blockchains develop independently, so their tokens also travel along these separating paths by extension.&lt;/div&gt;	
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/47329261-afec-4cf7-840e-5eee0c70ba61&quot;&gt;$600 million&lt;/a&gt; taken by hackers from DeFi-platform Poly Network. The job exploited a tool offered by Poly Network that allows users to transfer tokens between different blockchains. [1]&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] Essentially a mini-computer on a single chip.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-08-10/chip-delivery-time-surpasses-20-weeks-in-no-sign-shortage-easing?srnd=premium-asia&quot;&gt;Chip lead times&lt;/a&gt; stretch to 20+ weeks. Microcontrollers [2] now take 26.5 weeks - a drastic departure from the usual 6-9 weeks - while power management chip lead times have reduced.&lt;/li&gt;

	&lt;li&gt;A number of articles that discuss Apple&apos;s announcement regarding CSAM:
		&lt;ul&gt;
			&lt;li&gt;&lt;a href=&quot;https://stratechery.com/2021/apples-mistake/&quot;&gt;Ben Thompson&lt;/a&gt; argues Apple is betraying its privacy values by confusing capability vs. policy, and cloud vs. on-device.&lt;/li&gt;
			&lt;li&gt;&lt;a href=&quot;https://www.thepullrequest.com/p/hunting-predators&quot;&gt;Antonio Garcia Martinez&lt;/a&gt; is skeptical about the reported false positive rate of the photo-matching algorithm(s), and how greater power implies greater responsibility (and greater attention to the means of channeling this reponsibility).&lt;/li&gt;
			&lt;li&gt;&lt;a href=&quot;https://www.hackerfactor.com/blog/index.php?/archives/929-One-Bad-Apple.html&quot;&gt;Neal Krawetz&lt;/a&gt; discusses the fool-not-so-proof photo-matching algorithms and the potential legal problems with Apple&apos;s detection and reporting pipeline.&lt;/li&gt;
		&lt;/ul&gt;

		Overall, Ben Thompson&apos;s delineation of capability vs policy, and cloud vs. on-device is spot-on. The general arguments of the anti-Apple-policy crowd can be separated into 1. the slippery slope that these photo-matching algorithms could lead to if the definition of problematic photos expands and 2. Apple&apos;s newfound power in accessing on-device data, coupled with reporting process, demands greater responsibilty (which will probably be ignored).  
	&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Tuesday, August 10, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] Paraxylene is a petrochemical used for making bottles and other daily necessities. In particular, it is used for polyvesters such as polyethylene terephthalate (PET).&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-08-09/china-s-new-oil-giants-flourish-in-xi-s-clean-energy-wave?srnd=premium-asia&quot;&gt;Second-generation oil&lt;/a&gt; companies flourish under Chinese vision for self-sufficiency. These Teapot 2.0 companies are more environmentally conscious (unlike their older Teapot 1.0 counterparts), primarily focused on using crude to make paraxylene [1] rather than diesel. As a result, they enjoy tangible advantages such as tax benefits and permission to import directly from crude suppliers like Saudi Arabia.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.prnewswire.com/news-releases/knowde-raises-72-million-series-b-led-by-coatue-301349281.html&quot;&gt;Knowde&lt;/a&gt;, an &quot;Amazon&quot; for chemical ingredients, raises $72 million in Series B round. The vast majority (~90%) of the products produced by chemical companies are sold to other manufacturers that combine these products into our common consumer goods. Knowde provides a digital marketplace for these manufacturers.&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Tuesday, August 3, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/03a693a7-0445-41dd-a7f3-c1b6f162e5ef&quot;&gt;Roaring consumer demand&lt;/a&gt; tests global supply chain resilience. Increases in shipping and production rates have trickled down to the consumer level. The main bottleneck seems to be transportation and shipping congestion.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-08-02/methane-leak-detected-near-gas-pipeline-that-supplies-china?srnd=green&quot;&gt;Methane plume&lt;/a&gt; spotted near natural gas pipeline in Kazakhstan. As methane is the second-largest contributor to global warming, methane emissions are often misreported by governments and oil/gas operators.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Saturday, July 31, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/amp/articles/subsidies-chips-china-state-aid-biden-11627565906&quot;&gt;Industrial policy&lt;/a&gt; returns to America. In an effort to compete with China&apos;s goals of industrial self-sufficiency, the Biden administration plans to pump government support into critical sectors such as semiconductors, batteries, specialized minerals and pharmaceutical ingredients. The U.S. has traditionally led in innovation but lagged in production, preferring to outsource to other countries - no longer. It now aims to gain more control over the overall supply chain. [1]&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] Renewable diesel fuel is made from various biomass sources, and is chemically the same as petroleum diesel fuel. It differs from biodiesel by production process.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[3] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.eia.gov/todayinenergy/detail.php?id=48916&quot;&gt;U.S. production capacity&lt;/a&gt; for renewable diesel may surge through 2024. [2] With incentives such as higher state/federal targets and attractive tax credits, many projects have been announced; the projected renewable diesel output by 2024 is 5.1 billion gallons per year. A possible downside is that feedstocks also double as food, so food prices may spike. [3]&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Friday, July 30, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://twitter.com/NatureBiotech/status/1420795633955389444?s=20&quot;&gt;Scientists&lt;/a&gt; develop DNA polymerase capable of encoding L-DNA. [1] &lt;div class=&quot;marginnote&quot;&gt;[1] DNA polymerase is the enzyme responsible for DNA replication. &lt;/div&gt;As far as essential molecules for life are concerned, they follow a curious phenomenon called homochirality; roughly speaking, objects have &quot;handedness&quot;. For example, most amino acids are L-chiral, while sugars are D-chiral. [2] &lt;div class=&quot;marginnote&quot;&gt;[2] Think: L = left, D = droit. L-chiral is left-handed, D-chiral is right-handed. The question of the homochirality&apos;s necessity is off-topic, but there seems to be some evidence that homochirality aids in information storage and reduces entropy barriers.&lt;/div&gt; DNA, as it occurs in nature, is D-chiral. However, Tsinghua scientists have re-engineered DNA polymerase to encode L-DNA, the mirror image of D-DNA. Furthermore, this L-DNA is resistant to biodegradation i.e. it remains amplifiable and sequenceable after a nontrivial span of time. The greater implications of this technology is an explosion of mirror biology systems, which could lead to fantastic solutions for climate change or other significant central issues. [3] &lt;div class=&quot;marginnote&quot;&gt;[3] Read the paper &lt;a href=&quot;https://www.nature.com/articles/s41587-021-00969-6&quot;&gt;here&lt;/a&gt;.&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Thursday, July 29, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] &lt;img src=&quot;/assets/img/spiderman_meme.jpeg&quot; style=&quot;width: 50%; &quot;&gt;&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/5bbaa89b-2e85-4c5f-b918-566e6712d273&quot;&gt;Biden&lt;/a&gt; expresses cyberwar concerns regarding Russia and China following recent cyberattacks on essential infrastructure systems. [1]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/771498b8-9457-462f-aee0-e32db14eea49&quot;&gt;Redwood Materials&lt;/a&gt;, an energy startup that aims to recycle lithium-ion automotive battery packs, raises $700M in funding round. Its greater goal is to close the EV supply chain loop in the U.S., given that the EV supply chain is also an issue of national security - China currently controls a significant portion of the supply chain.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] Opinion piece.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.foreignaffairs.com/articles/united-states/2021-07-29/chinas-sputnik-moment&quot;&gt;Dan Wang&lt;/a&gt;&apos;s article on how U.S. sanctions on China have inadvertently aligned China&apos;s state and private goals for self-sufficiency. [2] Many of China&apos;s leading companies have critically relied on American technologies (e.g. semiconductors). However, Washington&apos;s restrictive actions regarding Huawei and other major companies further incentivize industrial self-sufficiency - in other words, the U.S. may be feeding the Chinese fire.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Wednesday, July 28, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-26/u-k-faces-harder-route-to-net-zero-with-nuclear-plans-in-danger&quot;&gt;United Kingdom&lt;/a&gt;&apos;s nuclear ventures may unravel. In 2015, the U.K. teamed up with China General Nuclear Power Corp. (CGN) and Electricite de France SA to build three nuclear reactors - one of which is led by CGN. However, the political and power implications of a Chinese-led major infrastructure project may derail the U.K.&apos;s clean energy plans. [1]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/for-some-businesses-the-pandemic-boom-has-peaked-11627415603?mod=hp_lead_pos4&quot;&gt;Consumer behavior&lt;/a&gt; on track towards pre-pandemic patterns. Companies like UPS and 3M benefitted during the heights of the pandemic, but a reversion of consumer behaviors means slower growth.&lt;/li&gt;

	&lt;li&gt;A number of threads/articles that attempt to explain the Chinese regulatory surge:
		&lt;ul&gt;
			&lt;li&gt;&lt;a href=&quot;https://noahpinion.substack.com/p/why-is-china-smashing-its-tech-industry&quot;&gt;@noahpinion&lt;/a&gt;&apos;s &lt;i&gt;Why is China Smashing Its Tech Industry?&lt;/i&gt; claims that China&apos;s crackdown is in line with its goals regarding comprehensive national and geopolitical power.&lt;/li&gt;
			&lt;li&gt;&lt;a href=&quot;https://twitter.com/ruima/status/1420060835313971200&quot;&gt;@ruima&lt;/a&gt;&apos;s thread on China&apos;s wanting to be a &quot;manufacturing-based superpower&quot;.&lt;/li&gt;
			&lt;li&gt;&lt;a href=&quot;https://lillianli.substack.com/p/let-the-bullets-fly-for-a-while&quot;&gt;@lillianmli&lt;/a&gt;&apos;s &lt;i&gt;Let The Bullets Fly For A While&lt;/i&gt;. She contends that China&apos;s recent crackdown fits into its regulatory pattern innovate-then-regulate, making the hot question not &quot;why?&quot;, but &quot;why now?&quot;.&lt;/li&gt;
		&lt;/ul&gt;
		The common theme running through all these arguments is a point made in Dan Wang&apos;s &lt;a href=&quot;https://danwang.co/2020-letter/&quot;&gt;2020 letter&lt;/a&gt;: China&apos;s future is focused on industralizing, manufacturing, and the real economy.
	&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Tuesday, July 27, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] Read this &lt;a href=&quot;https://twitter.com/pretentiouswhat/status/1419528716284137477&quot;&gt;tweet&lt;/a&gt; for further information.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[2] China has poor uranium reserves compared to its thorium reserves, so this is great for them.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.livescience.com/china-creates-new-thorium-reactor.html&quot;&gt;China&lt;/a&gt; nears completion of a Thorium Molten Salt Reactor (MSR). Thorium as found in nature (Th-232) is fertile, but Th-232 transmutes into the fissile Uranium-233 when bombarded with high-energy neutrons. In a Thorium MSR, thorium is dissolved into liquid salt, which is then sent into a reactor chamber. The liquid salt mixture ensures that the excess neutrons are high-energy neutrons, hence effectively &apos;waking the thorium beast&apos;. The MSR&apos;s implications include low water requirements and effective usage of fertile elements. [1][2]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/world/china/china-says-standstill-us-china-relations-due-us-treating-china-imaginary-enemy-2021-07-26/&quot;&gt;U.S.-China&lt;/a&gt; relations still sour after Deputy Secretary of State Wendy Sherman&apos;s visit. China demands that the U.S. not suppress or infringe upon its development, system of socialism, and state sovereignty.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] From Matt Levine&apos;s Moneystuff newsletter.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/8a328af4-b8f2-48c5-82a9-d7dc1c345e1c&quot;&gt;Banks&lt;/a&gt; lending more to high net worth clients. On the bank side, the logic is that they would rather lend to people who are likely to pay them back; for the wealthy, they want to liquidate their assets without paying taxes. [3]&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Friday, July 23, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/pg-e-in-reversal-to-bury-power-lines-in-fire-prone-areas-11626905920&quot;&gt;PG&amp;E&lt;/a&gt; to underground ten thousand miles of its power lines. Over the past few years, PG&amp;E&apos;s powerlines have ignited many California wildfire from powerlines touching trees or branches. The project will cost an estimated $20 billion; despite the hefty cost, the financial consequences of further wildfires are greater, affecting some past wildfire victims that were compensated for their losses with PG&amp;E shares. [1]&lt;/li&gt; 

	&lt;div class=&quot;marginnote&quot;&gt;[2] Opinion piece from Reuters.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[3] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/article/uk-column-russell-electricity-japan/column-japans-power-plan-will-rattle-coal-lng-exporters-especially-australia-idUSKBN2ES0PQ&quot;&gt;Japan&apos;s&lt;/a&gt; latest energy policy may shake up liquefied natural gas (LNG) and coal producers. [2] Japan has kept up steady demand for LNG and coal, but its new policy aiming to ramp up renewable and nuclear energy use may affect Australia especially, who is a major exporter in LNG and coal. [3]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/bidens-china-policy-borrows-from-trump-and-adds-allies-to-raise-pressure-11626958800?mod=hp_lead_pos6&quot;&gt;U.S.&apos;s&lt;/a&gt; confrontational approach to China to be tested. The U.S. Deputy Secretary of State, Wendy Sherman, flies to China this weekend for the first face-to-face meeting in three months.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Thursday, July 22, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[2] I mentioned during the Coop ransomware attacks that implementing technology is hard. A potential weakpoint here is then pipeline operators resorting to third-parties to construct their security systems. An attack on the third-party alone could be enough to hit a number of critical targets (although security systems are probably built bespoke so as to prevent standardization).&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/new-pipeline-cybersecurity-requirements-issued-by-biden-administration-11626786802&quot;&gt;Biden administration&lt;/a&gt; releases cybersecurity measures for U.S. pipelines. The directive also disclosed previously classified information that China, between 2011 and 2013, had compromised many U.S. oil and natural gas pipeline systems. In light of the Colonial Pipeline ransomware attack and the alleged Chinese cyberattack on Microsoft, the Biden administration appears to be taking a more active stance on protecting critical infrastructure systems. [1][2]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/49210a4e-17ed-4a2e-a986-4efcadc7f342&quot;&gt;Nord Stream 2&lt;/a&gt; deal between U.S. and Germany complete. The deal includes various checks on Moscow and aid to Ukraine. Ukraine and Poland are unhappy, asserting that the pipeline is a serious security threat.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-21/tesla-strikes-deal-with-top-miner-bhp-over-nickel-supplies&quot;&gt;Tesla&lt;/a&gt; makes a deal for nickel with BHP. With nickel being a key component in lithium-ion batteries, BHP and Tesla aim to make the battery supply chain more transparent.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/272259b0-8e98-4b49-8047-f4b8a2d33e95&quot;&gt;Saudi Aramco&lt;/a&gt; confirms $50m ransomware &quot;attack&quot;. A cyber extortionist allegedly had copies of stolen data and demanded a $50m ransom. However, Aramco denies that any of its systems were compromised.&lt;/li&gt;

&lt;/ol&gt;

&lt;h3&gt;Wednesday, July 21, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] Many current Russian gas pipelines connected to Europe run through Ukraine, who subsequently rakes in cash from transit fees.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/amp/articles/u-s-german-deal-on-russian-natural-gas-pipeline-expected-soon-11626813466&quot;&gt;US and Germany&lt;/a&gt; near an agreement regarding the completion of the Nord Stream 2 (NS2) gas pipeline. The Russian pipeline, which runs from Russia to Germany through the Baltic Sea, has been controversial considering its potential geopolitical and strategic implications. On the American side, the pipeline may increase Europe&apos;s already-high dependence on Russia for gas and allow the Kremlin to further bully Ukraine. [1] On the German and Russian side, the NS2 will double the amount of natural gas directly transported to Germany. Despite their continued opposition, the Americans concluded that bruising relations with Germany and other allies are a greater consequence. Under the agreement, U.S. and Germany will invest in Ukraine&apos;s green infrastructure and ensure that Ukraine continues to receive its transit fees. [2]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/7b678988-53d1-4a52-8866-28f109e88d79&quot;&gt;Delta variant&lt;/a&gt; hitting southeast Asia, affecting the semiconductor industry. SEA, especially Malaysia and Vietnam, has a major role in the production process (e.g. packaging, testing). The rapidly spreading Delta variant lands another punch, reducing worker capacity due to lockdowns and clogging up shipment schedules.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-20/bezos-poised-for-milestone-space-trip-on-blue-origin-rocket&quot;&gt;Bezos&lt;/a&gt; touches down. Topics about the environment regarding frequent space explorations, social inequality, and space economy have entered the forefront of people&apos;s minds.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/f4ca5b20-baaf-4c66-8c8f-6aea9937cf58&quot;&gt;Unilever&lt;/a&gt; pulls Ben &amp; Jerry&apos;s from occupied Palestinian territories. Some, like Israel&apos;s foreign minister Yair Lapid, have deemed this action as anti-semitic. Ben &amp; Jerry&apos;s is known to be a brand with strong ethical values, having supported Black Lives Matter and other social protests.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Tuesday, July 20, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/energy/opec-meets-agree-oil-supply-boost-prices-rise-2021-07-18/&quot;&gt;OPEC+&lt;/a&gt; agrees to release more oil into the market. UAE&apos;s baseline has increased by 332,000 bpd, and both Russia and Saudi Arabia&apos;s baselines have rise by 500,000 bpd. With more supply, the question of how demand will develop remains ambiguous - arguments for both bullish and bearish narratives are convincing. [1]&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] This is following the NSO Group&apos;s Pegasus spyware incident, where several journalists and human rights activists were targetted. Check out July 4th&apos;s news post for more.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/898e14b1-a4e1-4443-8d9a-8b5ff5238396&quot;&gt;Apple&lt;/a&gt; under scrutiny for refusing to collaborate with rivals on the cybersecurity front. Apple&apos;s &quot;closed-ecosystem&quot; approach means that the tech giant is tight-lipped about its vulnerabilities and updates. [2]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/pool-summer-airbnb-11626380408?mod=lead_feature_below_a_pos1&quot;&gt;Swimply&lt;/a&gt;, a platform for renting private pools. [3] &lt;div class=&quot;marginnote&quot;&gt;[3] An AirBnB type service.&lt;/div&gt; Pool hosts are hunting for the best pool maintenance prices, as disruptions in the pool-chemical supply chain due to the pandemic have caused prices to spike. Some hosts are even looking for more eco-friendly ways to maintain their pools.&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Saturday, July 17, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/nasa-hubble-space-telescope-back-11626476087?mod=hp_lead_pos9&quot;&gt;Hubble Space Telescope&lt;/a&gt; back in action. The entire Science Instrument Command and Data Handling (SIC&amp;DH) unit was switched out.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/cities-try-to-phase-out-gas-stovesbut-cooks-are-pushing-back-11626514200?mod=hp_lead_pos8&quot;&gt;Cooking industry&lt;/a&gt; holding onto gas stovetops. With many American cities aiming to reduce carbon emissions, gas-fired stovetops are (weirdly enough) becoming a point of contention. Some chefs are pushing back, arguing that electric setups have overall higher-cost and cannot achieve the same texture and flavor. Others are open to more environmentally friend possibilities.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/25840348-b0a4-46f1-8eb0-3de3bb70d8d9&quot;&gt;Tin prices&lt;/a&gt; on a high this past week. Used for solder in circuit boards, power shortages due to heatwaves and political unrest (in Myanmar, the third largest tin producer) have decreased supply while demand continues to rise.&lt;/li&gt; 

&lt;/ol&gt;

&lt;h3&gt;Friday, July 16, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[2] I&apos;ve covered the events leading up to this compromise, so read below for further information.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/world/middle-east/saudi-arabia-uae-reach-compromise-oil-output-deal-opec-source-2021-07-14/&quot;&gt;Compromise&lt;/a&gt; between Saudia Arabia and UAE regarding oil supply is on the horizon. [1][2]&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/2d034271-fcd7-4977-9d50-13bc048e6084&quot;&gt;US-China relations&lt;/a&gt; remain cold. China refused to grant the US Deputy Secretary of State Wendy Sherman permission to meet with her counterpart Le Yucheng. This rebuttal comes four months after the bitter Alaska talks between Yang Jiechi and Antony Blinken; China&apos;s expectation then was most likely to &quot;reset&quot; US-China relations with the Biden administration, but each side had fundamental disagreements over the other&apos;s values and behaviors.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/features/2021-07-14/moderna-mrna-targets-hiv-cancer-flu-zika-after-covid-vaccine&quot;&gt;Moderna&lt;/a&gt; wants to use mRNA technology against other diseases. Undoubtedly a heroic breakthrough in modern medicine, mRNA vaccines bypass the protein construction phase required in making traditional vaccines. Once injected, the immune cell constructs the protein from reading the mRNA and the immune system subsequently develops antibodies matching this protein. [3] &lt;div class=&quot;marginnote&quot;&gt;[3] In traditional vaccines, the protein is constructed first, &lt;b&gt;then&lt;/b&gt; inserted into the body.&lt;/div&gt;Due to the modularity of mRNA vaccines, they are easy to produce, and reliable at that. With these advantages, Moderna believes that mRNA vaccines could be used to combat HIV/AIDS, Zika, and other viral diseases.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/intel-is-in-talks-to-buy-globalfoundries-for-about-30-billion-11626387704?mod=hp_lead_pos1&quot;&gt;Intel&lt;/a&gt; might buy GlobalFoundries Inc., a US-based semiconductor manufacturer.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-15/square-building-new-bitcoin-inspired-financial-services-business&quot;&gt;Square&lt;/a&gt; looking to launch a Bitcoin DeFi platform.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Thursday, July 15, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/chinas-national-emissions-trading-set-to-begin-11626247709&quot;&gt;China&lt;/a&gt; prepares to launch emissions trading program. Having been delayed by the COVID-19 pandemic, the program will launch this Friday, involving around two thousand companies that make up a seventh of global carbon emissions from fossil-fuel combustion. In contrast to other cap and trade systems that impose an absolute cap, this program will allow for some margin depending on previous years&apos; performances. [1]&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/5d8c128a-1212-4abf-b0f4-c65d7409109b&quot;&gt;Mega oil/gas pipelines&lt;/a&gt; in danger. The trend of regulatory crackdowns and increasing costs of building pipelines is forcing oil/gas industries to re-think how hydrocarbons should be transported. Some are researching novel ways of transporting them (e.g. cooking into solid pucks and transporting via road or rail), while others are expanding into renewable energy and biofuel projects. [2]&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] From Matt Levine&apos;s Moneystuff newsletter.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.argaam.com/en/article/articledetail/id/1481346&quot;&gt;Saudi Arabia&lt;/a&gt; is considering issuing green bonds in Q4 this year. [3]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-14/netflix-plans-to-offer-video-games-in-expansion-beyond-films-tv?srnd=premium-asia&quot;&gt;Netflix&lt;/a&gt; looking to expand into the video game space, hence offering something that its competitors - Disney+, Amazon, Hulu, etc. - do not have yet. Furthermore, Netflix will offer these games at no further cost.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[4] Twitter is a good source of information and no one can convince me otherwise.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://twitter.com/red_dilettante/status/1414648055136198669&quot;&gt;Cuba&apos;s protests&lt;/a&gt; and what might actually be going on. The tweet author suggests three factors: &quot;the Leninist government&apos;s years-long crisis of legitimacy, the causes of recent hyper-inflation, the mixed politics of dissidents&quot;. Regarding the second factor, the government essentially tried to reform their two-currency system to match their plans for a tourist economy, but COVID made tourism evaporate. [4]&lt;/li&gt;

&lt;/ol&gt;

&lt;h3&gt;Wednesday, July 14, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/features/2021-07-13/bitcoin-miners-building-rigs-must-navigate-world-of-crypto-power-hunting?srnd=premium-asia&quot;&gt;Cryptocurrency miners&lt;/a&gt; looking to cheaper and renewable energy sources. With the Chinese government banning cryptocurrency mining over alleged illicit coal mining, many have been forced to seek power for their mining operations elsewhere. One favorite destination is Kazakhstan, where miners can get electrocity for as low as 3 cents per kilo-watt hour. However, Kazakhstan&apos;s power potential is waning, given its popularity and its lack of development in energy capacity over the past twenty years. Additionally, some miners are using this opportunity to investigate greener energy sources.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[1] Michael Foucault said: &quot;Where there is power, there is resistance, and yet, or rather consequently, this resistance is never in a position of exteriority in relation to power.&quot; I first encountered this relationship between attacker, defender, and customer in cryptography. From the defender&apos;s (Amazon, Ebay, etc.) point of view, you don&apos;t want your service to be too easy to penetrate. But you also don&apos;t want to increase the friction so much that people are discouraged from using your service entirely. &lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/return-scams-jump-as-fraudsters-exploit-e-commerce-boom-11626168601?mod=hp_lista_pos4&quot;&gt;E-commerce fraud&lt;/a&gt; - a weird exploding world amidst the pandemic. Fraudsters are taking advantage of the difficulties in proof of correct delivery and reception; for example, in the &quot;item not received&quot; fraud, a fraudster will falsely claim that he/she did not receive his/her ordered item, consequently receiving a refund. E-commerce frauds have taken off during the pandemic especially, for social distancing measures have decreased contact between delivery drivers and receivers. Some fraud operations even provide step-by-step manuals for committing these frauds. At the same time, fraud prevention services are working overtime to create effective deterrence methods. [1]&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Tuesday, July 13, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/3fcb2065-1952-4985-9f74-547eb4c57bfb&quot;&gt;Houston&lt;/a&gt; considering its post-oil future. Despite rising crude prices, shale producers are holding back on production to return capital back to investors as demand recovers. Given that the oil and gas shares of Houston&apos;s GDP have fallen dramatically in recent years, many Houston energy companies are looking favorably upon renewable energy such as wind power. Bobby Tudor of Tudor, Pickering, Holt &amp; Co also points to emerging technologies such as carbon and hydrogen capture. [1]&lt;/li&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[3] Carbon leakage is when companies move their environmentally un-safe operations out of tough climate regulation zones. Resource shuffling is when companies sell their environmentally friendly products to tough climate regulation zones, and sell their unfriendly products to places with weaker regulations.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/67b020ea-a82e-423e-a100-6d4df94d77ce&quot;&gt;Aluminum producers&lt;/a&gt; seek to be excluded from the first phase of the EU&apos;s carbon border adjustment mechanism (CBAM). CBAM would replace the existing emissions trading system, which compensates the aluminum industry for any carbon-related electricity costs. Aluminum producers claim that including aluminium in the CBAM will do nothing to prevent carbon leakage, or resource shuffling by competitors.[2][3]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-13/japan-lunar-council-urges-action-to-secure-lead-in-space-economy&quot;&gt;Japan&apos;s Lunar Industry Vision Council&lt;/a&gt; (LIVC) urges state to remain competitive in new space race. LIVC - which includes Sony and Nissin - submitted a white paper to Japan&apos;s space policy minister Shinji Inoue, claiming that Japan must be active if it is to be included in later lunar industrialization. &lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Saturday, July 10, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/c207e1c4-8978-4482-8b4d-c353acce33d1&quot;&gt;Executive order&lt;/a&gt; signed by U.S. President Joe Biden. The order aims to &quot;promote competition in the American economy&quot; [1] &lt;div class=&quot;marginnote&quot;&gt;[1] &lt;a href=&quot;https://www.whitehouse.gov/briefing-room/statements-releases/2021/07/09/fact-sheet-executive-order-on-promoting-competition-in-the-american-economy/&quot;&gt;Press release&lt;/a&gt; of the order.&lt;/div&gt;, and is in line with Biden&apos;s mission to redistribute centralized power to the smaller peripheries. The 72 total guidelines include vigorous enforcement of antitrust laws, greater scruntiny of mergers, and prevention of equipment manufacturers from restricting DIY repairs. [2] &lt;div class=&quot;marginnote&quot;&gt;[2] This &lt;a href=&quot;https://twitter.com/kendraschaefer/status/1413506152088834061?s=20&quot;&gt;tweet&lt;/a&gt; claims that this order is very similar to that of Chinese regulators in recent years. The similarities are really striking.&lt;/div&gt;&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-09/ethiopia-mega-dam-millions-in-sudan-egypt-caught-in-gerd-fight&quot;&gt;Grand Ethiopian Renaissance Dam&lt;/a&gt; (GERD) continues to spark clashes with neighboring countries Sudan and Egypt. Given that both Sudan and Egypt depend on the Nile for farming and energy generation, GERD&apos;s second phase of filling threatens to sour relations even further. For one, the GERD retains silt, but a momentary opening of its lower gates in November caused heavy silt to block the turbines of Sudan&apos;s Roseires dam - this lead to power outages throughout Sudan. Drought and flooding remain principal concerns also.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/09/new-york-city-biometrics-law/&quot;&gt;Biometrics&lt;/a&gt; law goes into effect on Friday in New York. The state will be requiring businesses that collect biometric information to post signs informing customers about how their data will be collected. Although weaker than Illinois&apos; Biometric Information Privacy Act, which allows residents to sue for any collection without consent, New York seems to be making steps to ensure a more deliberate approach to biometric data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Friday, July 9, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/fe8fc690-ce95-4622-95ac-e43ab261164d&quot;&gt;United States&lt;/a&gt; looking to decrease their dependence on China for rare earths. Important rare earths include cerium, lanthanum, and neodynium, which are used to make fuel cells or batteries. However, although rare earths are necessary for many green technologies such as electric cars, obtaining them can be a environmentally damaging process. [1]&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.eia.gov/todayinenergy/detail.php?id=48616&quot;&gt;Drought&lt;/a&gt; affects California&apos;s hydroelectric generation. Melting mountain snowpack normally acts as a natural source for hydroelectric generation, but most of it either melted earlier in the spring or was absorbed by dry soils and streams. Hydroelectric generation is therefore expected to be lower than that of previous years.[2]&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-08/tencent-uses-facial-recognition-to-ban-kids-gaming-past-bedtime&quot;&gt;Tencent&lt;/a&gt; rolls out facial recognition to curb gaming addiction. Minors will not be able to game after 22:00 and before 08:00; furthermore, refusing face verification implies immediate minor status, hence making a seemingly airtight system for preventing minors from gaming.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Thursday, July 8, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] In &lt;i&gt;Seeing Like A State&lt;/i&gt;, James C. Scott mentions scientific forestry and how the compression of natural complexity into a more administrative, controlled form led to uncontrollable setbacks. However, as similar as these pollination robots sound to scientific forestry, technology can be used to bypass artificial forcing-complexity-into-boxes altogether (given that the algorithm can recognize natural complexity). Nonetheless, whether this forced intrusion into the natural feedback loop between insects and plant reproduction is also ignoring levels of complexity is worth thinking about. &lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/buzz-off-bees-pollination-robots-are-here-11625673660?cx_testId=200&amp;cx_testVariant=cx_9&amp;cx_artPos=1#cxrecs_s&quot;&gt;Pollination robots&lt;/a&gt; might become a common sight on agricultural farms. With the declining insect population, farmers are looking for ways to decrease their dependence on bees to pollinate their crops. Arugga AI Farming, an AI-first-farming-second startup based in Israel, creates robots that identify and pollinate flowers that are ready for pollination. The robots would work faster than bee and human workers while offering more control over variables. [1]&lt;/li&gt;

	
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-07/covid-origins-mirrors-sars-introduction-from-animals-study-says&quot;&gt;Animal contagion&lt;/a&gt; may explain the COVID-19 pandemic. Edward Holmes and 19 other researchers released a pre-print that hypothesizes that animal contagion is to blame for the pandemic. [2] &lt;div class=&quot;marginnote&quot;&gt;[2] I didn&apos;t read the original paper, but &lt;a href=&quot;https://zenodo.org/record/5075888#.YOZUKRMzbDK&quot;&gt;here&lt;/a&gt; it is if you are curious.&lt;/div&gt; The authors claim that Wuhan coincidentally became the epicenter, it having a large population and being a nexus point for multiple animal markets. The anti-Chinese charged suspicion of the lab-leak theory is also probabilistically dismissed, as Wuhan coincidentally houses a biosafety lab also.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] Part of the appeal of Clubhouse is the FOMO aspect - you miss &quot;juicy&quot; tips by not being on the platform. Is Tiktok trying to leverage FOMO? At the same time, though, I wonder if this will introduce friction to the fairly frictionless content creation system that Tiktok has (content creation on Tiktok is almost genetic, with templates and filters that can be easily replicated and altered).&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/07/tiktok-resumes-job-applications/&quot;&gt;Applying for a job&lt;/a&gt; on TikTok becomes a reality. Part of Tiktok&apos;s effort to connect creators with brands, applicants can submit Tiktok video resumes to apply for positions such as social media manager or sales representative. [3]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/d4146bb5-896b-4f1f-b5f8-930cb2bfb729&quot;&gt;Increase in coffee prices&lt;/a&gt; might trickle down to wholesale and retail. With a big question mark surrounding consumer demand, coffee prices at the consumer level could be affected. On the supply side, bottlenecks caused by a shortage of containers and ships may lead to growers and exporters holding on to their reserves, exacerbating the price increase. For the most part, coffee buyers are protected by forward contracts, but problems will emerge once these contracts expire.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Wednesday, July 7, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/7ac0e691-665f-4328-8b29-ee4883068e80&quot;&gt;Pentagon&lt;/a&gt; cancels the Joint Enterprise Defense Infrastructure contract (JEDI) with Microsoft. The contract originally marked the Department of Defense&apos;s (DoD) attempt to move from physical to cloud-based operations. In 2019, President Donald Trump placed the contract on hold for complaints of favouritism towards Amazon. The winner was announced to be Microsoft later in the year. Amazon subsequently appealed, claiming that Trump was biased against them given Trump&apos;s known dislike of Bezos-owned The Washington Post.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[1] Just goes to show much culture/context matters. I think people regularly forget that cultural difference can rarely be overcome directly - you can attempt to abstract them away, however.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-06/landscape-of-rooftop-panels-looms-as-crowded-japan-boosts-solar?srnd=premium-asia&quot;&gt;Japan&lt;/a&gt; aims for greater solar capacity by 2030. Along with the large anti-nuclear sentiment, Japan is seeking alternative methods to lower their dependence on eco-malicious fossil fuels such as coal and natural gas. [1]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/06/yandex-self-driving-group-partners-with-grubhub-to-bring-robotic-delivery-to-college-campuses/&quot;&gt;Robotic delivery&lt;/a&gt; may become a regular occurrence on college campuses. Russian tech giant Yandex annouced their partnership with Grubhub to deliver food to college students using self-driving delivery robots. Yandex is using a robot-first approach to the delivery problem.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] We do not naturally make enzymes to alleviate allergic reactions, for example.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[3] Enzymes are made of strings of amino acids that are bunched up in a very specific way.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/06/allozymes-looks-to-upend-chemical-manufacturing-with-rapid-enzyme-engineering-and-5m-seed/&quot;&gt;Allozymes&lt;/a&gt; wants to make enzyme bio-engineering more efficient. Enzymes that are not naturally occurring are often required [2], but inventing new enzymes is no easy task. Given how complex amino acid chains are [3], the traditional process takes long and costs much, adopting an almost naive &quot;find the needle in the haystack&quot; approach. Allozyme instead aims to make the enzyme selection process more self-contained, hence reducing costs and increasing efficiency.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Tuesday, July 6, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/c9746fbc-7a90-4c9a-9a52-30b44475aa9a&quot;&gt;The OPEC+&lt;/a&gt; fiasco continues. The next OPEC meeting has been called off, with another date hopefully to be set in the near future. The markets responded accordingly - brent crude oil rose to $77.16 per barrel.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[1] Eugene Wei&apos;s &lt;a href=&quot;https://www.eugenewei.com/blog/2020/8/3/tiktok-and-the-sorting-hat&quot;&gt;amazing essay&lt;/a&gt; claims that TikTok has used AI to abstract away cultural differences between the U.S. and China. However, this new article overrates the AI&apos;s power; IMHO, the very specific deployment of AI to erase cultural boundaries is what made Tiktok powerful, not solely the AI itself. &lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/bed7cba1-db7a-49c7-9d57-06fd19e14e10&quot;&gt;Bytedance&lt;/a&gt; starts selling the artificial intelligence technology that powers TikTok.[1]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/05/vw-offloads-bugatti-to-rimac-to-form-new-ev-company-bugatti-rimac/&quot;&gt;Rimac&lt;/a&gt; takes over Bugatti, with a controlling 55% share of the new Bugatti-Rimac. Rimac is an electric hypercar manufacturer with a single model: the Nevera.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/771f6d40-ecd2-4855-8193-d0550f1d2e3d&quot;&gt;Other US-listed Chinese tech groups&lt;/a&gt; targeted by China&apos;s cyberspace division. Didi Chuxing was removed from Chinese app stores shortly after its IPO for improperly collecting user data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Monday, July 5, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-04/saudi-arabia-sticks-to-demand-for-opec-extension-through-2022&quot;&gt;UAE&lt;/a&gt; refuses to budge on OPEC+ deal. Long-term allies UAE and Saudi Arabia are expressing their differences, with the former refusing to sign the deal that extends into 2022. UAE agreed to the current baseline in April 2020, but an extension of this baseline into 2022 is undesirable, as they have invested heavily in production capacity.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-04/chinese-astronauts-make-first-space-walk-outside-new-station&quot;&gt;Astronauts&lt;/a&gt; make first spacewalk outside of the Tiangong space station, installing cameras and other equipment on the Tianhe core module. The space station is part of the larger Tiangong project, beginning in 1992. [1] &lt;div class=&quot;marginnote&quot;&gt;[1] Apparently NASA refused China&apos;s involvement in the ISS&apos;s construction, which heightened China&apos;s incentives to build their own station.&lt;/div&gt;The launch of the Tianhe core module in April 2021 marks the start of the program&apos;s deployment.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] Lithography machines are used to etch circuit designs on silicon wafers. ASML uses a special brand of lithography called extreme ultraviolet lithography, which uses extreme ultraviolet for etching.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.nytimes.com/2021/07/04/technology/tech-cold-war-chips.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage&quot;&gt;ASML&lt;/a&gt; lies at the center of the semiconductor war. The Dutch semiconductory company possesses the technology to push past the performance barrier. However, being a company that has a near-monopoly on EUV lithography machines [2], the company is getting shut into the US-China war.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.project-syndicate.org/commentary/fed-sanguine-inflation-view-recalls-arthur-burns-by-stephen-s-roach-2021-05&quot;&gt;Stephen S. Roach&lt;/a&gt; talks about inflation. This is an older opinion piece from May 2021, but highlights bigger themes surrounding inflation this time around. Roach remembers Arthur Burns&apos; philosophy surrounding inflation - Burns believed that price trends are largely affected by transitory noise that has nothing to do with monetary policy. Hence he sought to reduce the consumer price index (CPI) to products that were independent of noise, at one point excluding energy and food products. Only when the resulting CPI continued to rise at a double-digit rate did Burns acknowledge America&apos;s inflation problem.

	&lt;p&gt;This highlights the dangers of cherry-picking: picking &quot;nicer&quot; data points to see a desired result like a lower significance level. Furthermore, the delineation between COVID and normalcy can be dangerous; treating COVID as merely a passing phase encourages a Burns-esque philosophy that increases in various consumer products will disappear once COVID is &quot;over&quot;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Sunday, July 4, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] I think this highlights a number of interesting things. 
		&lt;ol&gt;
			&lt;li&gt;Power graphs (i.e. the distribution of power) are important, and they are not always equivalent to the distribution of agents.&lt;/li&gt;
			&lt;li&gt;We sometimes forget, but software - and technology in general - is hard. The costs of implementing in-house systems for small/medium-sized businesses that are not in the tech space are far greater than just getting a third-party to take care of everything.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/a8e7c9a2-5819-424f-b087-c6f2e8f0c7a1&quot;&gt;Hackers&lt;/a&gt; hit Kaseya, an IT software provider, to spread ransomware. Consequently, many of Sweden&apos;s Coop grocery chain stores, which depend on Kaseya for their cash register and self-checkout systems, remained closed. This attack suggests the dependence of many small/medium-sized companies on centralized third parties for their IT service needs. [1]&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/03/digital-violence-nso-group-spyware/&quot;&gt;Digital Forensics&lt;/a&gt; presents an &lt;a href=&quot;https://www.digitalviolence.org/#/&quot;&gt;interactive timeline&lt;/a&gt; of the state terror caused by the NSO Group&apos;s spyware &lt;i&gt;Pegasus&lt;/i&gt;. The spyware is allegedly capable of activating personal smartphone cameras and microphones, hence leveraging the widespread use of smartphones to gain intelligence. &lt;div class=&quot;marginnote&quot;&gt;[2] I&apos;m working on a thesis that as things become more niche, the need to standardize and homogenize increases also. Pegasus seems to be exploiting this thesis, if true.&lt;/div&gt; Digital Forensics developed digitalviolence to counter the use of spyware like Pegasus towards questionable, unethical purposes. [2]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/legal/litigation/case-watch-toxic-commercial-weedkiller-fuels-new-mass-tort-2021-07-02/&quot;&gt;Herbicide&lt;/a&gt; suggested to be linked to Parkinson&apos;s disease could fuel a new mass tort. Paraquat is a known toxic herbicide marketed by Syngenta and previously sold by USA&apos;s Chevron. A tort may arise from data showing that chronic exposure to paraquat may be correlated with Parkinson&apos;s disease.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Saturday, July 3, 2021&lt;/h3&gt;

&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://twitter.us18.list-manage.com/track/click?u=92fd2e3ec7962cda008f0732a&amp;id=23e3ae9eb6&amp;e=1c03e42b50&quot;&gt;Cars&lt;/a&gt; are back in fashion. [1] A primary reason for this resurgence in car ownership is the pandemic, which discouraged the use of public transportation (due to fears of viral transmission). Used car prices and waiting times for driving tests have risen. An interesting observation is that, at least in America, the total distance travelled by car is not even at pre-pandemic levels. I would guess that this discrepancy emerges from the prevalence of remote work and other stay-at-home policies.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/energy/opec-seeks-oil-policy-consensus-uae-demands-changes-2021-07-02/&quot;&gt;UAE&lt;/a&gt; wants OPEC+ to let it raise oil output. OPEC+ is looking to increase oil supply in the market to ease a surge in oil prices. However, UAE have rejected this new deal on the grounds that their maximum production capacity has increased since 2018, so the proposed deal will cut into their production revenues more than it will for Russia and Saudi Arabia.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] I really wonder how our ancestors would react if they saw 1. people getting information from these massively vast social graphs and 2. the mediators of these social graphs policing the information.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[4] It&apos;d be cool to build a GAN with the Twitter misinformation labeller as the adversary.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/01/twitter-colorful-misinformation-labels/&quot;&gt;Twitter&lt;/a&gt; rolls out misinformation labels. The labels color-code misleading information, and include a link that takes you to a curated landing page with verified information. The role of social networks in the spread of information (true or un-true alike) is interesting. Social graphs have always been a source of information, but the constant verification of information with social origins is the fascinating bit. [3] I suspect a lot of Americans will call this a rights infringement, but that&apos;s a separate conversation altogether. [4]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/china-india-move-tens-of-thousands-of-troops-to-the-border-in-largest-buildup-in-decades-11625218201?cx_testId=200&amp;cx_testVariant=cx_10&amp;cx_artPos=0#cxrecs_s&quot;&gt;Tensions&lt;/a&gt; between China and India rise. Both sides have been fortifying their positions, continuing the escalating narrative since the June 2020 Galwan Valley clash.&lt;/li&gt;

&lt;/ol&gt;


&lt;h3&gt;Friday, July 2, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/sustainable-business/us-natgas-companies-put-hydrogen-test-2021-07-01/&quot;&gt;Natural gas suppliers&lt;/a&gt; [1] experiment with hydrogen blending. Hydrogen blending is a method of reducing carbon emissions from natural gas by mixing hydrogen into natural gas pipelines. Apparently, there is such a thing as &lt;a href=&quot;https://en.wikipedia.org/wiki/Green_hydrogen&quot;&gt;green hydrogen&lt;/a&gt;, which is hydrogen produced from low-carbon sources (as opposed to the usual fossil fuel sources). The high cost of green hydrogen production seems to be the barrier.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.eia.gov/todayinenergy/detail.php?id=48536&quot;&gt;California&lt;/a&gt; [2] and other major regions are at risk of energy shortages this summer. Peak energy demand may increase given the heat wave. California is most at risk because it relies on energy imports, and solar energy output falls in the late afternoon.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] A similar mechanism to why increasing interest rates theoretically increases the home currency&apos;s value?&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-01/global-tax-overhaul-wins-broad-consensus-but-obstacles-remain&quot;&gt;130 countries&lt;/a&gt; back a plan for setting the minimum global corporate tax rate at 15%. The minimum tax rate is meant to discourage multinational corporations from shifting their money to lower-tax countries.[3] Lower-tax countries such as Ireland have benefitted from the influx of investments from multi-national companies, so a final agreement could have major repurcussions.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[4] I don&apos;t know anything about the housing market, so this is an excuse for me to learn more about it.&lt;/div&gt;
	
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/features/2021-07-01/world-s-fastest-property-price-surge-since-financial-crisis-sparks-bidding-wars&quot;&gt;Property valuations&lt;/a&gt; are soaring globally. I wonder if the pandemic has made people more selective about their living spaces.[4]&lt;/li&gt;

	

	
&lt;/ol&gt;

&lt;h3&gt;Thursday, July 1, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] For one, the presenter doesn&apos;t gesticulate nor have any facial expressions.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/06/30/look-out-language-teachers-a-synthetic-human-could-be-about-to-take-your-job/&quot;&gt;Hour One&lt;/a&gt;, a startup that creates photo-real presenters, signs a deal with Berlitz. COVID has led to greater acceptance of technological solutions for traditionally offline things, but I&apos;m curious how this turns out. I do think this has the most application in language learning, since being able to see a mouth sounding out words can be crucial for learning pronunciation. However, I&apos;m doubtful that putting a face to text is really more engaging.[1] Also text has the greatest signal density, so by using this presenter, the signal density goes down significantly.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] &quot;It&apos;s called the little bighorn. That&apos;s smart, Mark.&quot;&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/df4f6e4c-9ee5-47ff-aa8f-8c4c328a1f39&quot;&gt;Zipline&lt;/a&gt;, a drone delivery startup, raises $250 million. Zipline first attacked the African space by making strides in medical supply lines, using drones to deliver vaccines and medicines to local hospitals. Using drones may not usher in a technological revolution, but it reminds me of Carlota Perez&apos;s &lt;i&gt;Technological Revolutions and Financial Capital&lt;/i&gt;; in her model, technological revolutions generally start at certain cores (US, Britain, etc.), then diffuse to the peripheries. This appears to demonstrate the opposite phenomenon. Ghana&apos;s restrictions against drones may have been more lax, hence providing a nice training ground for Zipline.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/3de60b3b-fa83-466a-b38d-d5cde3680b36&quot;&gt;Robinhood&lt;/a&gt; faced to pay $70 million to resolve allegations that it misled customers. The psychology surrounding money is fascinating; at least in America, having a lot of money is synonymous with a good life, and I think the behaviors seen on Robinhood are in-line with this thesis.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Wednesday, June 30, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/06/29/the-engineering-daring-that-led-to-the-first-chinese-personal-computer/&quot;&gt;Sinotype III&lt;/a&gt; and how Chinese characters arrived onto the digital computing scene. Sinotype III was an Apple II with a modified word processor and operating system; it was developed (strangely?) by the Graphic Arts Education and Research Foundation (GARF), an American foundation.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/chinas-didi-raises-4-billion-us-ipo-source-2021-06-29/&quot;&gt;Didi Chuxing&lt;/a&gt;, a Chinese ride-hailing company, to raise an expected $4 billion in US IPO. A lot of Didi&apos;s ride-sharing comes from taxi-hailing rather than private-car sharing (similar to Kakao Taxi), and with Meituan and Ele.me already popular in the Chinese food delivery market, it&apos;ll be interesting to see how Didi grows.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/06/29/family-app-life360-announces-2-1m-investment-round-from-celebs-and-influencers/&quot;&gt;Life360&lt;/a&gt; announces a new investment round of $2.1 million. This trend of wanting to know the locations of your families and friends is an interesting one. I remember it started back in the 2010s with Snapchat announcing their locations feature. Since then, I&apos;ve had multiple friends ask me to install location-sharing apps, but I never understood why someone would willingly give away his/her location. Maybe the placement of a social graph in physical space?&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-06-29/sam-altman-s-worldcoin-will-give-free-crypto-for-eyeball-scans?srnd=technology-vp&quot;&gt;Worldcoin&lt;/a&gt;, a cryptocurrency that requires a retinal scan for identification. As the name suggests, it strives for universal basic income by bypassing government regulation with technology. The relationship between centralized and decentralized systems, and the analogous tension between centralized and decentralized &lt;b&gt;power&lt;/b&gt; comes to mind here.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-06-29/eighty-year-old-japanese-firm-may-be-key-to-next-gen-chip-tech&quot;&gt;Disco Corps&lt;/a&gt; is a Japanese company that specializes in precision grinding and dicing equipment, which are essential for semiconductor manufacturing.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">[Completed.]</summary></entry><entry><title type="html">Files &amp;amp; Books</title><link href="http://localhost:3000/2021/06/12/files/" rel="alternate" type="text/html" title="Files &amp;amp; Books" /><published>2021-06-12T00:00:00-05:00</published><updated>2021-06-12T00:00:00-05:00</updated><id>http://localhost:3000/2021/06/12/files</id><content type="html" xml:base="http://localhost:3000/2021/06/12/files/">&lt;h3&gt;Notes&lt;/h3&gt;
&lt;h4&gt;Ongoing&lt;/h4&gt;
&lt;ul&gt;
&lt;/ul&gt;
&lt;h4&gt;Completed&lt;/h4&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/fractal_notes.pdf&quot;&gt;Fractal Notes for UChicago REU 2021&lt;/a&gt; - I&apos;ve found fractals to be very helpful for thinking about complexity.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STAT_244.pdf&quot;&gt;STAT 24400&lt;/a&gt; - Winter 2020, taught by Dr. Rina Barber&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STAT245_cheatsheet.pdf&quot;&gt;STAT 24500&lt;/a&gt; - Spring 2021, taught by Dr. Chao Gao&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STAT343_review.pdf&quot;&gt;STAT 34300&lt;/a&gt; - Autumn 2022, taught by Dr. Rina Barber&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STAT261_review.pdf&quot;&gt;STAT 26100&lt;/a&gt; - Autumn 2022, taught by Dr. Wei Bao Wu&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Papers&lt;/h3&gt; 
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STEDI_iam_decoder_paper8.pdf&quot;&gt;(in progress) How Stedi Uses Automated Reasoning for Access Control Policy Verification&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/edi_fingerprint.pdf&quot;&gt;An Exploration of Transaction Set Identification&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/finite_pop.pdf&quot;&gt;Evolutionary Game Dynamics: Finite Populations&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/mmar.pdf&quot;&gt;Overview of The Multifractal Model of Asset Returns&lt;/a&gt; - UChicago REU 2021&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/ergodic_entropy.pdf&quot;&gt;Equivalent Notions of Entropy Under Ergodicity&lt;/a&gt; - UChicago REU 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Books&lt;/h3&gt;

&lt;h4&gt;Completed - 2022&lt;/h4&gt;
	&lt;ul&gt;
		&lt;li&gt;The Origin of Wealth: The Radical Remaking of Economics and What it Means for Business and Society - Eric D. Beinhocker&lt;/li&gt;
		&lt;li&gt;Global Capitalism: Its Fall and Rise in the Twentieth Century - Jeffrey A. Frieden&lt;/li&gt;
		&lt;li&gt;The Courage to be Disliked - Ichiro Kishimi &amp; Fumitake Koga&lt;/li&gt;
		&lt;li&gt;The Wandering Earth - Cixin Liu&lt;/li&gt;
		&lt;li&gt;Debt: The Last 5,000 Years - David Graeber &lt;i&gt;(dropped)&lt;/i&gt;&lt;/li&gt;
		&lt;li&gt;The Remains of the Day - Kazuo Ishiguro&lt;/li&gt;
		&lt;li&gt;The Ascent of Money: A Financial History of the World - Niall Ferguson&lt;/li&gt;
		&lt;li&gt;Animal Spirits: How Human Psychology Drives The Economy, and Why It Matters For Global Capitalism - George A. Akerlof &amp; Robert J. Shiller&lt;/li&gt;
		&lt;li&gt;The Perfectionists: How Precision Engineers Created The Modern World - Simon Winchester&lt;/li&gt;
	&lt;/ul&gt;
&lt;h4&gt;Completed - 2021&lt;/h4&gt;
	&lt;ul&gt;
		&lt;li&gt;The Innovation Delusion: How Our Obsession with the New Has Disrupted the Work That Matters Most - Lee Vinsel &amp; Andrew L. Russell&lt;/li&gt;
		&lt;li&gt;101 Things I Learned in Architecture School - Matthew Frederick&lt;/li&gt;
		&lt;li&gt;The Box: How the Shipping Container Made the World Smaller and the World Economy Bigger - Marc Levinson&lt;/li&gt;
		&lt;li&gt;Leonardo da Vinci - Walter Isaacson&lt;/li&gt;
		&lt;li&gt;What Has Passed Shall in Kinder Light Appear - Baoshu (Li Jun)&lt;/li&gt;
		&lt;li&gt;The Psychology of Money: Timeless Lessons on Wealth, Greed, and Happiness - Morgan Housel&lt;/li&gt;
		&lt;li&gt;Samsung Rising: The Inside Story of the South Korean Giant That Set Out to Beat Apple and Conquer Tech - Geoffrey Cain&lt;/li&gt;
		&lt;li&gt;The Everything Store: Jeff Bezos and The Age of Amazon - Brad Stone&lt;/li&gt;
		&lt;li&gt;Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed - James C. Scott&lt;/li&gt;
		&lt;li&gt;Normal People - Sally Rooney&lt;/li&gt;
		&lt;li&gt;Warfighting - U.S. Marine Corps&lt;/li&gt;
		&lt;li&gt;Upheaval - Jared Diamond&lt;/li&gt; 
		&lt;li&gt;The (Mis)behavior of Markets - Benoit Mandelbrot, Richard Hudson&lt;/li&gt;
		&lt;li&gt;The Biggest Bluff: How I Learned to Pay Attention, Master Myself, and Win - Maria Konnikova&lt;/li&gt;
		&lt;li&gt;On China - Henry Kissinger&lt;/li&gt;
	&lt;/ul&gt;</content><author><name></name></author><summary type="html">Notes Ongoing Completed Fractal Notes for UChicago REU 2021 - I&apos;ve found fractals to be very helpful for thinking about complexity. STAT 24400 - Winter 2020, taught by Dr. Rina Barber STAT 24500 - Spring 2021, taught by Dr. Chao Gao STAT 34300 - Autumn 2022, taught by Dr. Rina Barber STAT 26100 - Autumn 2022, taught by Dr. Wei Bao Wu</summary></entry><entry><title type="html">Audio Fingerprinting</title><link href="http://localhost:3000/2021/01/03/fingerprint/" rel="alternate" type="text/html" title="Audio Fingerprinting" /><published>2021-01-03T00:00:00-06:00</published><updated>2021-01-03T00:00:00-06:00</updated><id>http://localhost:3000/2021/01/03/fingerprint</id><content type="html" xml:base="http://localhost:3000/2021/01/03/fingerprint/">My project: &lt;a href=&quot;https://github.com/alikiki/audioplay&quot;&gt;audioplay&lt;/a&gt;
&lt;h2&gt;Basic Signal Processing Basics&lt;/h2&gt;
	&lt;h4&gt;Waves&lt;/h4&gt;
		&lt;p&gt;When a note is played on the piano, the sound that emanates from the piano and reaches our ears is actually a wave. As soon as the piano hammer hits the string, the air surrounding the piano string vibrates i.e. areas of low air pressure and high air pressure are created. These vibrations travel throughout the air and causes our ear drums to vibrate, causing other ear components to send signals to the brain. These signals are interpreted as &quot;sound&quot;. [1]&lt;/p&gt;

		&lt;div class=&quot;marginnote&quot;&gt;[1] Not all waveforms look like the clean sine and cosine functions; any look at the stock market shows that waveforms can look extremely complicated.&lt;/div&gt;

	&lt;h4&gt;Fourier Transform&lt;/h4&gt;
		&lt;p&gt;Any waveform can be broken down into its constituent sine and cosine waves (also called simple sinusoids), for which the terms &lt;b&gt;frequency&lt;/b&gt; and &lt;b&gt;amplitude&lt;/b&gt; are more well-defined. For example, the waveform below is pretty irregular, so it&apos;s difficult to observe any concrete properties by just looking at it. Luckily, the Fourier Transform decomposes the waveform into simple sinusoids, from which we can observe the various frequencies that make up the waveform.&lt;/p&gt;
		&lt;img src=&quot;/assets/data/fingerprint/fourier.png&quot; style=&quot;display: block; width: 60%;margin-left: auto; margin-right: auto;&quot;&gt;


		&lt;div class=&quot;marginnote&quot;&gt;[2] The best of both time and frequency worlds is encapsulated in the spectrogram, which we introduce soon.&lt;/div&gt;
		&lt;p&gt;Notice how the faded orange, green, and pink waves are just simple sinusoids with uniform frequencies and waveforms. &lt;b&gt;This is huge.&lt;/b&gt; The Fourier Transform implies that waveforms can be represented in terms of both time and frequency, each representation showing what the other cannot. [2] The canonical representation of waves is time-based, but notice that the temporal representation offers no obvious frequency information. On the other hand, the frequency representation likewise eliminates time information.&lt;/p&gt;



&lt;h3&gt;Digitalization&lt;/h3&gt;
	&lt;p&gt;The above introduced the necessary tools and information for audio fingerprinting. However, music in its digital form places critical restrictions on these tools - as always, knowing when not to apply certain tools is just as important as knowing when to apply them, so we explore the relevant consequences of digitalization below.&lt;/p&gt;
	&lt;h4&gt;Sampling and its consequences&lt;/h4&gt;
		&lt;p&gt;The difference between analog and digital music may be slight for the casual music listener, but most audiophiles will acknowledge that analog music sounds warmer and deeper compared to its counterpart - this is due to the process in which analog music is converted to digital music, or sampling. Sampling transforms a continuous signal into a discrete one. [3] As a simple illustration, making a connect-the-dots puzzle from a line drawing is essentially sampling - a conversion from the continuous to the discrete.&lt;/p&gt;

		&lt;div class=&quot;marginnote&quot;&gt;[3] &quot;Discrete&quot; and &quot;continuous&quot; are just math jargon for &quot;chopped up&quot; and &quot;smooth&quot;, respectively.&lt;/div&gt;

		&lt;img src=&quot;/assets/data/fingerprint/sampling.png&quot;&gt;

		&lt;p&gt;The specific mechanics of sampling consists of marking dots on a continuous signal at a fixed speed (called the sampling rate). But how fast should the sampling rate be such that the continuous signal can be deduced from its discretized form? We should hope that it&apos;s fast enough so that the deduction is easier than completing a connect-the-dots puzzle. A slow sample rate keeps us guessing, but a fast sample rate clearly shows the shape of the continuous signal.&lt;/p&gt;

		&lt;img src=&quot;/assets/data/fingerprint/fast_slow.png&quot;&gt;

		&lt;p&gt;However, as the sample rate increases, the memory demand also increases. We&apos;re faced with a clear dilemma here: either increase the sample rate and risk taking up too much memory, or decrease the sample rate and lose data to the point that the original signal is impossible to deduce. Where is the Goldilock&apos;s zone of sampling rates for music?&lt;/p&gt;

		&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem&quot;&gt;The Nyquist-Shannon sampling theorem&lt;/a&gt; states that given a signal that contains no frequencies greater than \( B \) hertz, a sample rate of at least \( 2B \) samples per second is sufficient to reconstruct the continuous signal from its discretized form. Since human ears can only hear up to around 20 kHz, a sample rate of 40 kHz should be sufficient. This is where the magic number of 44.1 kHz comes from. &lt;/p&gt;

		&lt;p&gt;It seems as though we have resolved our digitalization problem. Sampling at 44.1 kHz is sufficient, so now we should be free to analyze digital waveforms. Unfortunately, the Fourier Transform in its current form poses a major obstruction.&lt;/p&gt;

	&lt;h4&gt;Discrete Fourier Transform (DFT)&lt;/h4&gt;

		&lt;p&gt;The Fourier transform can extract frequency information only from continuous waveforms. Luckily, a discrete version of the Fourier Transform exists, the aptly-named &lt;a href=&quot;https://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;&gt;Discrete Fourier Transform&lt;/a&gt;. Without going into too much technical detail, the DFT essentially bins frequency information. The frequencies that the DFT extracts are grouped together; the exact specifications of these groupings is determined by the sampling rate and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Window_function&quot;&gt;window&lt;/a&gt;. We skip the technical details in this layman&apos;s introduction.&lt;/p&gt;

&lt;h2&gt;audioplay: reverse-search&lt;/h2&gt;
	&lt;p&gt;As an exercise, let&apos;s explore what is important in song identification. As studied above, the essential components of any song are frequency, time, and amplitude. 

	&lt;ul&gt;
		&lt;li&gt;&lt;b&gt;Amplitude.&lt;/b&gt; Clearly, the loudness (a.k.a. amplitude) is irrelevant; a song played at 10 dB is still the same song when played at 100 dB. &lt;/li&gt;
		&lt;li&gt;&lt;b&gt;Frequency.&lt;/b&gt; A song pitched up several semitones is noticeably different from the original song, so frequency should be important. Of course, the original song will be recognizable, but to say that the pitched-up version is the same as the original would be a stretch.&lt;/li&gt;
		&lt;li&gt;&lt;b&gt;Time.&lt;/b&gt; Similarly, a sped-up song is not the same as the original. However, overall song length is generally useless; instead, the time differences between specific moments are meaningful identifiers. For example, if a note hits earlier than you expect, then you will probably second-guess your memory. &lt;/li&gt;
	&lt;/ul&gt;
	&lt;/p&gt;
	&lt;p&gt;With these ideas in mind, the project consists of two components: fingerprinting and recognition. The first transforms sound into a common medium that we can work with, and the second pertains to the song identification itself. In other words, fingerprinting is the mise en place, while recognition is the actual cooking.&lt;/p&gt;
	
	&lt;p&gt;The project&apos;s general overview is as follows:
	&lt;ol&gt;
		&lt;li&gt;Fingerprinting&lt;/li&gt;
			&lt;ul&gt;
				&lt;li&gt;Spectrogram generation&lt;/li&gt;
				&lt;li&gt;Peak detection&lt;/li&gt;
				&lt;li&gt;Peak association&lt;/li&gt;
				&lt;li&gt;Hashing&lt;/li&gt;
			&lt;/ul&gt;
		&lt;li&gt;Recognition&lt;/li&gt;
			&lt;ul&gt;
				&lt;li&gt;Fingerprinting&lt;/li&gt;
				&lt;li&gt;Scoring&lt;/li&gt;
			&lt;/ul&gt;
	&lt;/ol&gt;&lt;/p&gt;

&lt;h3&gt;Fingerprinting&lt;/h3&gt;
&lt;h4&gt;I. Spectrogram generation&lt;/h4&gt;
	&lt;p&gt;The spectrogram combines time and frequency information into a single representation, hence returning the time information lost through the Fourier transform. Because it compresses all the necessary information of a song into a single representation, it is the focal point of the entire fingerprinting process.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/spectro.jpg&quot; style=&quot;display: block; width: 80%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[4] Notice that the lower frequencies i.e. bass frequencies are especially high in amplitude. But when you listen to a song, the bass doesn&apos;t sound drastically louder than any of the other frequencies, so why are the bass frequencies so red? Because human ears are more sensitive to certain frequencies than others, sound engineers boost up the bass to make the overall track more &quot;even&quot;. For the curious, look up &quot;psychoacoustics&quot;.&lt;/div&gt;

	&lt;p&gt;The x-axis, y-axis, and the color show the time, frequency, and amplitude, respectively. Just for intuition, notice that the above spectrogram shows red spikes, where nearly all the frequencies in the entire range are firing off at a high amplitude (this is probably the snare of the song). [4]&lt;/p&gt;

	

&lt;h4&gt;II. Peak detection&lt;/h4&gt;
	&lt;p&gt;For our purposes, the spectrogram is an image, and an image is just an \(n \times n\) array of numbers, where each pixel is a cell in the array. The time and frequency determines the cell&apos;s location, and the amplitude dictates its value. A peak is then a (time, frequency) pair with the greatest amplitude value among the pixels surrounding it.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/spec_draw1.png&quot; style=&quot;display: block; width: 80%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;p&gt;To identify peaks, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_morphology&quot;&gt; binary morphology&lt;/a&gt;. [5] The process is as follows:
		&lt;div class=&quot;marginnote&quot;&gt;[5] Binary morphology has a lot of applications. Your drawing program probably uses binary morphology to make lines thinner or thicker, for example.&lt;/div&gt;
		&lt;ol&gt;
			&lt;li&gt;Place a 19-by-19 pixel square \(S\) over any part of the image.&lt;/li&gt;
			&lt;li&gt;Identify the pixel with the maximum value within \(S\). Call this maximum value \(M\).&lt;/li&gt;
			&lt;li&gt;Set all the pixel values within \(S\) to \(M\).&lt;/li&gt;
			&lt;li&gt;Apply the following rule to every pixel in \(S\): for any pixel \(P\) in \(S\),
				&lt;ul&gt;
					&lt;li&gt;If \(P = M\), color the pixel white.&lt;/li&gt;
					&lt;li&gt;If \(P \neq M\), color the pixel black.&lt;/li&gt;
				&lt;/ul&gt;&lt;/li&gt;
		&lt;/ol&gt;&lt;/p&gt;



	&lt;p&gt;For example, executing steps 1 through 3 gives the &quot;Mask Applied&quot; image [6], and the last step produces the &quot;Identified Peaks&quot; image. In the &quot;Mask Applied&quot; image, there are distinct squares, which are the 19-by-19 pixel squares described above. The &quot;Identified Peaks&quot; image shows scattered white dots on a black background, where the white dots are the maximal value pixels.&lt;/p&gt;

		&lt;img src=&quot;/assets/data/fingerprint/morphology.png&quot;&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[6] Values in this grayscale image range from 0 to 255, with 0 being black and 255 being white.&lt;/div&gt;
	&lt;p&gt;Finding the peaks is then just a matter of gathering the locations i.e. (time, frequency) pairs of the white dots. You may notice that by identifying peaks, we&apos;ve lost all amplitude information, since the white dots show only the locations of the peaks. Thankfully, amplitude is not important for our purposes. &lt;/p&gt;

&lt;h4&gt;III. Peak Association&lt;/h4&gt;
	&lt;p&gt;Surprisingly, this list of peak locations is actually sufficient for our goal to &quot;reverse-search&quot; audio. But we are greedy. Search needs to be fast, and using the vanilla list of peak locations would take absurdly long. By associating peaks with each other, we get higher information density and easy searchability at the expense of more computation. Not a bad deal.&lt;/p&gt;

	&lt;p&gt;The intuition behind peak association follows from the principle that the number of links between things increases faster than the number of things themselves. For example, there is only 1 possible link between 2 things, but 45 possible links between just 10 things.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/scale.png&quot; style=&quot;display: block; width: 100%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;p&gt;This means that from just a list of 10 peaks, we can extract 4 times as much information. The bang for our buck increases quadratically too, so more peaks means higher information density.&lt;/p&gt;

	&lt;h5&gt;III-a. Anchors + Target Zones&lt;/h5&gt;
		&lt;p&gt;However, chaotically pairing all possible peaks won&apos;t be fruitful - we need order in the chaos to extract the most information possible. The pairing method is as follows:
			&lt;ol&gt;
				&lt;li&gt;Order the peaks by time.&lt;/li&gt;
				&lt;li&gt;Choose a peak, and call it the anchor \(A\).&lt;/li&gt;
				&lt;li&gt;Choose the target peaks by selecting the 10 consecutive peaks \(\{ T_1, ..., T_{10} \}\) that come after skipping the first 3 peaks that are immediately after \(A\). For example, if \(A\) is the 10th peak, then the target peaks are the 14th, 15th, ..., 23rd peaks.&lt;/li&gt;
				&lt;li&gt;Form the pair associations \( \{ (A, T_1), ..., (A, T_{10})\}. \)&lt;/li&gt;
				&lt;li&gt;Repeat by setting every peak as the anchor, insofar as the 10 target peaks exist. For example, if only 10 peaks exist, then the 10th peak cannot be an anchor peak because there are no more potential target peaks.&lt;/li&gt;
			&lt;/ol&gt;	
		&lt;/p&gt;
		&lt;img src=&quot;/assets/data/fingerprint/peak_assoc.png&quot; style=&quot;display: block; width: 75%;margin-left: auto; margin-right: auto;&quot;&gt;

		&lt;p&gt;The anchor point brings order to the chaos. Because every peak in the target zone is linked to a common anchor point, every target peak has a &quot;common denominator&quot;. In terms of ease of information access, there is a stark difference between saying &quot;John is friends with Anne, Bob, and Clark&quot;, versus &quot;Anne is friends with Bob, Clark is friends with John, etc&quot;.&lt;/p&gt;

&lt;h4&gt;IV. Hashing&lt;/h4&gt;
	&lt;p&gt;Unfortunately, this list of peak pairs isn&apos;t easily searchable, so we need a method to compress each pair into an easily searchable form. Hashing associates each pair with a unique string, akin to how license plates uniquely identify cars. Input an anchor-target pair into the hashing function, and out comes a string that uniquely identifies that pair.&lt;/p&gt;

	&lt;p&gt;Recall that we hypothesized that only time difference and frequency are essential for identification. While the specifics of the hashing function aren&apos;t important, what we input into our hashing function reflects our hypothesis.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/hash.png&quot; style=&quot;display: block; width: 70%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;p&gt;Remembering that every peak is a (time, frequency) point, for every anchor-target pair \( (A, T) \), identify:
		&lt;ul&gt;
			&lt;li&gt;\(F_{\text{anchor}} := \) frequency of anchor peak.&lt;/li&gt;
			&lt;li&gt;\(F_{\text{target}} := \) frequency of target peak.&lt;/li&gt;
			&lt;li&gt;\(T_{\text{offset}} := \) time difference between target peak and anchor peak.&lt;/li&gt;
		&lt;/ul&gt;	

	The triple \( (F_{\text{anchor}}, F_{\text{target}}, T_{\text{offset}}) \) becomes the input. By hashing every anchor-target pair, we generate a list of strings that uniquely identifies the song and is easily searchable. Having effectively translated music into organized, searchable data, this concludes the fingerprinting process.&lt;/p&gt;

&lt;h3&gt;Recognition&lt;/h3&gt;
	&lt;p&gt;The sous chef has mise en place-d all the ingredients, and now it&apos;s time to cook. For the rest of this section, assume that we have a database full of song fingerprints. Also, we refer to the audio clip that needs to identified as the &quot;id clip&quot;, and the database songs as the &quot;db clips&quot;. For example, we need to check if the id clip matches any of the db clips.&lt;/p&gt;
&lt;h4&gt;I. Fingerprinting&lt;/h4&gt;
	&lt;p&gt;Fingerprinting translates sound into a standardized form of data, so an audio clip must first be fingerprinted before recognition so that the clip can be analyzed. Fortunately, the same fingerprinting algorithms can be used for this stage, so nothing extra needs to be added.&lt;/p&gt;

&lt;h4&gt;II. Scoring&lt;/h4&gt;
	&lt;p&gt;As an exercise, let&apos;s study how similar the fingerprints generated from two different sources playing the same song are. Both sources should generate similar spectrograms, and hence similar peaks. Therefore, the anchor-target pairs should coincide, with the only possible dissimilarities being in time and amplitude. Nonetheless, the time &lt;b&gt;differences&lt;/b&gt; between the anchor-target pairs in both should be constant. In sum, the invariant information consists of the anchor-target pair frequencies and time differences, which are exactly the inputs of the hashing function.&lt;/p&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[7] It&apos;s unlikely that the clip for identification is the exact same clip that was used to fingerprint the song for the database. Hence, we need to work probabilistically: which song in the database has the highest probability of being the song in the clip?&lt;/div&gt;

	&lt;p&gt;Therefore, if the song in an audio clip matches a song in the database, then the hashes must coincide. Then identification simply becomes a matter of choosing the song that has the highest number of hash matches, and we are done! [7]&lt;/p&gt;
	
&lt;h3&gt;References and Further Reading&lt;/h3&gt;
	&lt;ol&gt;
		&lt;li&gt;The &lt;a href=&quot;/assets/data/fingerprint/shazam.pdf&quot;&gt;original Shazam paper.&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Will Drevo&apos;s &lt;a href=&quot;https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/&quot;&gt;dejavu&lt;/a&gt; project was invaluable and I heavily borrowed from the project source code.&lt;/li&gt;
		&lt;li&gt;Cameron Macleod&apos;s &lt;a href=&quot;https://github.com/notexactlyawe/abracadabra&quot;&gt;abracadabra&lt;/a&gt; also helped a lot.&lt;/li&gt;
	&lt;/ol&gt;</content><author><name></name></author><summary type="html">My project: audioplay Basic Signal Processing Basics Waves When a note is played on the piano, the sound that emanates from the piano and reaches our ears is actually a wave. As soon as the piano hammer hits the string, the air surrounding the piano string vibrates i.e. areas of low air pressure and high air pressure are created. These vibrations travel throughout the air and causes our ear drums to vibrate, causing other ear components to send signals to the brain. These signals are interpreted as &quot;sound&quot;. [1]</summary></entry></feed>